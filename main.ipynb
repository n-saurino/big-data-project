{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip\n",
    "#!pip install tqdm\n",
    "#!pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "import pyspark\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0745851c-98cb-468f-8422-527aa52bcb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    "    .setMaster(\"local[7]\")\\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .set(\"spark.eventLog.dir\", \"./logs\")\\\n",
    "    .set(\"spark.eventLog.gcMetrics.youngGenerationGarbageCollectors\", \"true\")\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0176948-f20c-45af-a761-428e1c21618d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Priyank:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[7]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x197a3ba6e50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=15000000', 'filename': 'arrest.json' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=15000000', 'filename': 'summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/bjcb-yee3.csv?$limit=15000000', 'filename': 'turnstile.csv' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=10000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "         if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "#arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)\n",
    "#complaint_rdd = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True)\n",
    "#summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)\n",
    "turnstile_rdd = spark.read.csv(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True, header=True,inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29ff0-4812-4fd5-9e3d-267f51352243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecddb38-1aa3-48c3-9225-908bb51b3800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "turnstile_rdd = turnstile_rdd.withColumn(\"date\", col(\"Date\").cast(DateType())).withColumn(\"time\", date_format(col(\"time\"), \"HH:mm:ss\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e27a521-c7ee-4f26-acfe-999f8e5bef34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "turnstile_rdd = turnstile_rdd.select(col('unit'), col('station'), col('date'), col('entries'), col('exits'))\n",
    "#turnstile_rdd.printSchema()\n",
    "#turnstile_rdd.toPandas() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fcf1d9c-c753-41c4-bb85-64d7b2749f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------+----------+--------+--------+\n",
      "|unit|       station|      date| entries|   exits|\n",
      "+----+--------------+----------+--------+--------+\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 6069026| 7074661|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|14197229|13704110|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 2056268| 5177036|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 2056268| 5177036|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|70294362|20274025|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 1806541| 2182381|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|14197229|13704110|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|70294362|20274025|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 6069026| 7074661|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|  694109|  207786|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 1806541| 2182381|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 4927946| 3748371|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|  694109|  207786|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|15598097| 6250334|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 4927946| 3748371|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|15598097| 6250334|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 3965537| 2697302|\n",
      "|R170|14 ST-UNION SQ|2018-12-28|11499477| 1255112|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 3965537| 2697302|\n",
      "|R170|14 ST-UNION SQ|2018-12-28| 3687618| 2644410|\n",
      "+----+--------------+----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True, nullValue='')\n",
    "turnstile_rdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028033bb-af8d-42ff-bcb5-e96c32df83ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station ID: integer (nullable = true)\n",
      " |-- complex_id: integer (nullable = true)\n",
      " |-- GTFS Stop ID: string (nullable = true)\n",
      " |-- Division: string (nullable = true)\n",
      " |-- Line: string (nullable = true)\n",
      " |-- station: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Daytime Routes: string (nullable = true)\n",
      " |-- Structure: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- North Direction Label: string (nullable = true)\n",
      " |-- South Direction Label: string (nullable = true)\n",
      " |-- ADA: integer (nullable = true)\n",
      " |-- ADA Direction Notes: string (nullable = true)\n",
      " |-- ADA NB: integer (nullable = true)\n",
      " |-- ADA SB: integer (nullable = true)\n",
      " |-- Capital Outage NB: string (nullable = true)\n",
      " |-- Capital Outage SB: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complex_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>40.775036</td>\n",
       "      <td>-73.912034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>40.770258</td>\n",
       "      <td>-73.917843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>40.766779</td>\n",
       "      <td>-73.921479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>40.761820</td>\n",
       "      <td>-73.925508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>40.756804</td>\n",
       "      <td>-73.929575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>517</td>\n",
       "      <td>40.525507</td>\n",
       "      <td>-74.200064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>518</td>\n",
       "      <td>40.522410</td>\n",
       "      <td>-74.217847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>519</td>\n",
       "      <td>40.519631</td>\n",
       "      <td>-74.229141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>522</td>\n",
       "      <td>40.512764</td>\n",
       "      <td>-74.251961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>523</td>\n",
       "      <td>40.516578</td>\n",
       "      <td>-74.242096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     complex_id   latitude  longitude\n",
       "0             1  40.775036 -73.912034\n",
       "1             2  40.770258 -73.917843\n",
       "2             3  40.766779 -73.921479\n",
       "3             4  40.761820 -73.925508\n",
       "4             5  40.756804 -73.929575\n",
       "..          ...        ...        ...\n",
       "491         517  40.525507 -74.200064\n",
       "492         518  40.522410 -74.217847\n",
       "493         519  40.519631 -74.229141\n",
       "494         522  40.512764 -74.251961\n",
       "495         523  40.516578 -74.242096\n",
       "\n",
       "[496 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_rdd = subway_rdd.withColumnRenamed(\"Complex ID\", \"complex_id\").withColumnRenamed(\"Stop Name\", \"station\").withColumnRenamed(\"GTFS Latitude\", \"latitude\").withColumnRenamed(\"GTFS Longitude\", \"longitude\")\n",
    "subway_rdd.printSchema()\n",
    "subway_rdd = subway_rdd.select(\"complex_id\", \"latitude\", \"longitude\")\n",
    "subway_rdd.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cfa8d12-46b6-443e-bf1f-9c1e39b2631c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "turnstile_rdd = turnstile_rdd.groupBy(['unit','station', 'date']) \\\n",
    "                           .agg(sum('entries').alias('entries'), sum('exits').alias('exits'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a60a01d-27b3-431b-82db-4b617bb11357",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+----------+-----------+-----------+\n",
      "|unit|        station|      date|    entries|      exits|\n",
      "+----+---------------+----------+-----------+-----------+\n",
      "|R156|BEDFORD PK BLVD|2018-12-28| 1730762526|   78056832|\n",
      "|R292|  BAYCHESTER AV|2018-12-28|  196532900|   20297276|\n",
      "|R550|     CITY / BUS|2018-12-28|   99005486|   81797186|\n",
      "|R334| CATHEDRAL PKWY|2018-12-26| 1090510336| 1782637664|\n",
      "|R222|    PARKCHESTER|2018-12-25| 1175301086|  645398588|\n",
      "|R203|          23 ST|2018-12-25|  310225202|  214048832|\n",
      "|R278|          25 ST|2018-12-24|  166487130|   57945786|\n",
      "|R452|          72 ST|2018-12-23|42282868696|55910954994|\n",
      "|R389|BRONX PARK EAST|2018-12-23|11926356972| 5263177644|\n",
      "|R157| NORWOOD 205 ST|2018-12-21| 2170967301| 1346719694|\n",
      "|R428|       BUHRE AV|2018-12-21|   44656706|   29820000|\n",
      "|R250|       GRAND ST|2018-12-21|  111085500|   94134580|\n",
      "|R414| HOWARD BCH JFK|2018-12-21|   28867693|    8000473|\n",
      "|R148|    PARKSIDE AV|2018-12-20|  236141035|  207208197|\n",
      "|R131|          23 ST|2018-12-20|18486965305|16041847600|\n",
      "|R196|  PROSPECT PARK|2018-12-19| 9668464760| 4957604421|\n",
      "|R378|      MYRTLE AV|2018-12-18|  243019729|  199275152|\n",
      "|R060|EASTN PKWY-MUSM|2018-12-18|  143241111|  155567854|\n",
      "|R414| HOWARD BCH JFK|2018-12-17|   28810600|    7988698|\n",
      "|R266|      HALSEY ST|2018-12-16|  606902378|  328149351|\n",
      "+----+---------------+----------+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sway_rdd = spark.read.csv(\"D:\\\\Bigdata_project\\\\remote_complex_lookup.csv\", header=True, inferSchema=True, ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True, nullValue='')\n",
    "sway_rdd = sway_rdd.select(\"complex_id\", \"remote\")\n",
    "turnstile_rdd.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5003ece3-c31b-457a-93cc-d9dfdf0f5119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = sway_rdd.join(subway_rdd, on='complex_id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88f257d1-58f7-4f5c-8844-7b3040d2d1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complex_id</th>\n",
       "      <th>unit</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>635</td>\n",
       "      <td>R001</td>\n",
       "      <td>40.702068</td>\n",
       "      <td>-74.013664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>635</td>\n",
       "      <td>R001</td>\n",
       "      <td>40.703087</td>\n",
       "      <td>-74.012994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>635</td>\n",
       "      <td>R001</td>\n",
       "      <td>40.702068</td>\n",
       "      <td>-74.013664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>635</td>\n",
       "      <td>R001</td>\n",
       "      <td>40.703087</td>\n",
       "      <td>-74.012994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>635</td>\n",
       "      <td>R001</td>\n",
       "      <td>40.702068</td>\n",
       "      <td>-74.013664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>477</td>\n",
       "      <td>R570</td>\n",
       "      <td>40.768799</td>\n",
       "      <td>-73.958424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>476</td>\n",
       "      <td>R571</td>\n",
       "      <td>40.777891</td>\n",
       "      <td>-73.951787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>476</td>\n",
       "      <td>R571</td>\n",
       "      <td>40.777891</td>\n",
       "      <td>-73.951787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>475</td>\n",
       "      <td>R572</td>\n",
       "      <td>40.784318</td>\n",
       "      <td>-73.947152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>475</td>\n",
       "      <td>R572</td>\n",
       "      <td>40.784318</td>\n",
       "      <td>-73.947152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1011 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      complex_id  unit   latitude  longitude\n",
       "0            635  R001  40.702068 -74.013664\n",
       "1            635  R001  40.703087 -74.012994\n",
       "2            635  R001  40.702068 -74.013664\n",
       "3            635  R001  40.703087 -74.012994\n",
       "4            635  R001  40.702068 -74.013664\n",
       "...          ...   ...        ...        ...\n",
       "1006         477  R570  40.768799 -73.958424\n",
       "1007         476  R571  40.777891 -73.951787\n",
       "1008         476  R571  40.777891 -73.951787\n",
       "1009         475  R572  40.784318 -73.947152\n",
       "1010         475  R572  40.784318 -73.947152\n",
       "\n",
       "[1011 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.withColumnRenamed(\"remote\", \"unit\")\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bb4a48d-8b78-4eec-92d0-35f3f9116d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+----------+-----------+-----------+----------+---------+----------+\n",
      "|unit|        station|      date|    entries|      exits|complex_id| latitude| longitude|\n",
      "+----+---------------+----------+-----------+-----------+----------+---------+----------+\n",
      "|R156|BEDFORD PK BLVD|2018-12-28| 1730762526|   78056832|       211|40.873244|-73.887138|\n",
      "|R292|  BAYCHESTER AV|2018-12-28|  196532900|   20297276|       443|40.878663|-73.838591|\n",
      "|R334| CATHEDRAL PKWY|2018-12-26| 1090510336| 1782637664|       155|40.800603|-73.958161|\n",
      "|R222|    PARKCHESTER|2018-12-25| 1175301086|  645398588|       366|40.833226|-73.860816|\n",
      "|R203|          23 ST|2018-12-25|  310225202|  214048832|       228|40.742878|-73.992821|\n",
      "|R278|          25 ST|2018-12-24|  166487130|   57945786|        31|40.660397|-73.998091|\n",
      "|R452|          72 ST|2018-12-23|42282868696|55910954994|       313|40.778453| -73.98197|\n",
      "|R389|BRONX PARK EAST|2018-12-23|11926356972| 5263177644|       425|40.848828|-73.868457|\n",
      "|R157| NORWOOD 205 ST|2018-12-21| 2170967301| 1346719694|       210|40.874811|-73.878855|\n",
      "|R428|       BUHRE AV|2018-12-21|   44656706|   29820000|       361| 40.84681|-73.832569|\n",
      "|R250|       GRAND ST|2018-12-21|  111085500|   94134580|       123|40.711926| -73.94067|\n",
      "|R414| HOWARD BCH JFK|2018-12-21|   28867693|    8000473|       198|40.660476|-73.830301|\n",
      "|R148|    PARKSIDE AV|2018-12-20|  236141035|  207208197|        43|40.655292|-73.961495|\n",
      "|R131|          23 ST|2018-12-20|18486965305|16041847600|       405|40.739864|-73.986599|\n",
      "|R196|  PROSPECT PARK|2018-12-19| 9668464760| 4957604421|        42|40.661614|-73.962246|\n",
      "|R378|      MYRTLE AV|2018-12-18|  243019729|  199275152|        97|40.697207|-73.935657|\n",
      "|R060|EASTN PKWY-MUSM|2018-12-18|  143241111|  155567854|       341|40.671987|-73.964375|\n",
      "|R414| HOWARD BCH JFK|2018-12-17|   28810600|    7988698|       198|40.660476|-73.830301|\n",
      "|R266|      HALSEY ST|2018-12-16|  606902378|  328149351|       129|40.695602|-73.904084|\n",
      "|R415|  BROAD CHANNEL|2018-12-15|    8182868|    1190853|       199|40.608382|-73.815925|\n",
      "+----+---------------+----------+-----------+-----------+----------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=turnstile_rdd.join(df, on='unit', how='inner').distinct()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "422a8899-ccda-4ee1-9382-ce249e995a57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df3 = df2.dropDuplicates([\"latitude\", \"longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "74cfb063-bd7a-4611-b1f4-97f0f6905cb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit</th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>complex_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>R312</td>\n",
       "      <td>W 8 ST-AQUARIUM</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>4326237</td>\n",
       "      <td>9156352</td>\n",
       "      <td>57</td>\n",
       "      <td>40.576127</td>\n",
       "      <td>-73.975939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R264</td>\n",
       "      <td>OCEAN PKWY</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>129405</td>\n",
       "      <td>75681</td>\n",
       "      <td>56</td>\n",
       "      <td>40.576312</td>\n",
       "      <td>-73.968501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R151</td>\n",
       "      <td>CONEY IS-STILLW</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>3309942</td>\n",
       "      <td>1284341</td>\n",
       "      <td>58</td>\n",
       "      <td>40.577422</td>\n",
       "      <td>-73.981233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R172</td>\n",
       "      <td>BRIGHTON BEACH</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>584713</td>\n",
       "      <td>283597</td>\n",
       "      <td>55</td>\n",
       "      <td>40.577621</td>\n",
       "      <td>-73.961376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R419</td>\n",
       "      <td>ROCKAWAY PARK B</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>100664041</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>40.580903</td>\n",
       "      <td>-73.835592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>R431</td>\n",
       "      <td>EASTCHSTER/DYRE</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>6431698</td>\n",
       "      <td>8285886</td>\n",
       "      <td>442</td>\n",
       "      <td>40.888300</td>\n",
       "      <td>-73.830834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>R117</td>\n",
       "      <td>V.CORTLANDT PK</td>\n",
       "      <td>2018-07-22</td>\n",
       "      <td>620922125</td>\n",
       "      <td>620916198</td>\n",
       "      <td>293</td>\n",
       "      <td>40.889248</td>\n",
       "      <td>-73.898583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>R367</td>\n",
       "      <td>233 ST</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>10031280</td>\n",
       "      <td>2355779</td>\n",
       "      <td>418</td>\n",
       "      <td>40.893193</td>\n",
       "      <td>-73.857473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>R444</td>\n",
       "      <td>NEREID AV</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>858204</td>\n",
       "      <td>280996</td>\n",
       "      <td>417</td>\n",
       "      <td>40.898379</td>\n",
       "      <td>-73.854376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>R145</td>\n",
       "      <td>WAKEFIELD/241</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>4619402</td>\n",
       "      <td>6464436</td>\n",
       "      <td>416</td>\n",
       "      <td>40.903125</td>\n",
       "      <td>-73.850620</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>474 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     unit          station        date    entries      exits  complex_id   \n",
       "0    R312  W 8 ST-AQUARIUM  2018-12-28    4326237    9156352          57  \\\n",
       "1    R264       OCEAN PKWY  2018-12-28     129405      75681          56   \n",
       "2    R151  CONEY IS-STILLW  2018-12-28    3309942    1284341          58   \n",
       "3    R172   BRIGHTON BEACH  2018-12-28     584713     283597          55   \n",
       "4    R419  ROCKAWAY PARK B  2018-12-28  100664041          0         203   \n",
       "..    ...              ...         ...        ...        ...         ...   \n",
       "469  R431  EASTCHSTER/DYRE  2018-12-28    6431698    8285886         442   \n",
       "470  R117   V.CORTLANDT PK  2018-07-22  620922125  620916198         293   \n",
       "471  R367           233 ST  2018-12-28   10031280    2355779         418   \n",
       "472  R444        NEREID AV  2018-12-28     858204     280996         417   \n",
       "473  R145    WAKEFIELD/241  2018-12-28    4619402    6464436         416   \n",
       "\n",
       "      latitude  longitude  \n",
       "0    40.576127 -73.975939  \n",
       "1    40.576312 -73.968501  \n",
       "2    40.577422 -73.981233  \n",
       "3    40.577621 -73.961376  \n",
       "4    40.580903 -73.835592  \n",
       "..         ...        ...  \n",
       "469  40.888300 -73.830834  \n",
       "470  40.889248 -73.898583  \n",
       "471  40.893193 -73.857473  \n",
       "472  40.898379 -73.854376  \n",
       "473  40.903125 -73.850620  \n",
       "\n",
       "[474 rows x 8 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b336e1bf-cba9-4146-877a-d8b978fa9cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "# create a window partitioned by station and ordered by date\n",
    "station_window = Window.partitionBy('station').orderBy('date')\n",
    "\n",
    "# create a new dataframe with aggregated entries and exits by station and date\n",
    "station_day_sum = (\n",
    "    df3.groupBy('station', 'date')\n",
    "      .agg(sum('entries').alias('entries'), sum('exits').alias('exits'),\n",
    "           first('latitude').alias('latitude'), first('longitude').alias('longitude'))\n",
    "      .withColumn('date', df3.date.cast('date'))\n",
    "      .withColumn('row_num', F.row_number().over(station_window))\n",
    ")\n",
    "\n",
    "# filter to only include the first row of each station (i.e. the earliest date for that station)\n",
    "station_day_sum = station_day_sum.filter(F.col('row_num') == 1).drop('row_num')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4e126c7-e7f4-43c7-abe9-d1cbaacf5a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station</th>\n",
       "      <th>date</th>\n",
       "      <th>entries</th>\n",
       "      <th>exits</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 AV</td>\n",
       "      <td>2018-07-28</td>\n",
       "      <td>18230852265</td>\n",
       "      <td>9759011769</td>\n",
       "      <td>40.730953</td>\n",
       "      <td>-73.981628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>103 ST</td>\n",
       "      <td>2018-02-05</td>\n",
       "      <td>353438446</td>\n",
       "      <td>192195754</td>\n",
       "      <td>40.799446</td>\n",
       "      <td>-73.968379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103 ST-CORONA</td>\n",
       "      <td>2018-09-18</td>\n",
       "      <td>402532124</td>\n",
       "      <td>411560198</td>\n",
       "      <td>40.749865</td>\n",
       "      <td>-73.862700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104 ST</td>\n",
       "      <td>2018-05-10</td>\n",
       "      <td>10108650071</td>\n",
       "      <td>5877256005</td>\n",
       "      <td>40.681711</td>\n",
       "      <td>-73.837683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110 ST</td>\n",
       "      <td>2018-02-15</td>\n",
       "      <td>201464538</td>\n",
       "      <td>195302500</td>\n",
       "      <td>40.795020</td>\n",
       "      <td>-73.944250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>WOODHAVEN BLVD</td>\n",
       "      <td>2018-09-23</td>\n",
       "      <td>251901586</td>\n",
       "      <td>162469082</td>\n",
       "      <td>40.733106</td>\n",
       "      <td>-73.869229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>WOODLAWN</td>\n",
       "      <td>2018-11-25</td>\n",
       "      <td>176711552</td>\n",
       "      <td>69992735</td>\n",
       "      <td>40.886037</td>\n",
       "      <td>-73.878751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>WTC-CORTLANDT</td>\n",
       "      <td>2018-12-18</td>\n",
       "      <td>4394348320</td>\n",
       "      <td>5415208</td>\n",
       "      <td>40.711835</td>\n",
       "      <td>-74.012188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>YORK ST</td>\n",
       "      <td>2018-12-13</td>\n",
       "      <td>245143240</td>\n",
       "      <td>201479832</td>\n",
       "      <td>40.701397</td>\n",
       "      <td>-73.986751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>ZEREGA AV</td>\n",
       "      <td>2018-05-18</td>\n",
       "      <td>326327752</td>\n",
       "      <td>9467061</td>\n",
       "      <td>40.836488</td>\n",
       "      <td>-73.847036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            station        date      entries       exits   latitude  longitude\n",
       "0              1 AV  2018-07-28  18230852265  9759011769  40.730953 -73.981628\n",
       "1            103 ST  2018-02-05    353438446   192195754  40.799446 -73.968379\n",
       "2     103 ST-CORONA  2018-09-18    402532124   411560198  40.749865 -73.862700\n",
       "3            104 ST  2018-05-10  10108650071  5877256005  40.681711 -73.837683\n",
       "4            110 ST  2018-02-15    201464538   195302500  40.795020 -73.944250\n",
       "..              ...         ...          ...         ...        ...        ...\n",
       "342  WOODHAVEN BLVD  2018-09-23    251901586   162469082  40.733106 -73.869229\n",
       "343        WOODLAWN  2018-11-25    176711552    69992735  40.886037 -73.878751\n",
       "344   WTC-CORTLANDT  2018-12-18   4394348320     5415208  40.711835 -74.012188\n",
       "345         YORK ST  2018-12-13    245143240   201479832  40.701397 -73.986751\n",
       "346       ZEREGA AV  2018-05-18    326327752     9467061  40.836488 -73.847036\n",
       "\n",
       "[347 rows x 6 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "station_day_sum.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4597c90f-0902-4018-a227-1bd917c9e008",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e08231-269f-4996-a0d9-d250583b9363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e4dec10-a7b8-4e81-8f56-9aad9ff39338",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 114.0 failed 1 times, most recent failure: Lost task 0.0 in stage 114.0 (TID 76) (Priyank executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# select the rows corresponding to the most recent 5 days\u001b[39;00m\n\u001b[0;32m     30\u001b[0m most_recent_5 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM station_day_sum WHERE date >= DATE_SUB(CURRENT_DATE(), 5)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mmost_recent_5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_circle_marker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# display the map\u001b[39;00m\n\u001b[0;32m     35\u001b[0m fmap\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart_1.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1380\u001b[0m, in \u001b[0;36mDataFrame.foreach\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforeach\u001b[39m(\u001b[38;5;28mself\u001b[39m, f: Callable[[Row], \u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the ``f`` function to all :class:`Row` of this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1361\u001b[0m \n\u001b[0;32m   1362\u001b[0m \u001b[38;5;124;03m    This is a shorthand for ``df.rdd.foreach()``.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;124;03m    >>> df.foreach(func)\u001b[39;00m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1380\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1748\u001b[0m, in \u001b[0;36mRDD.foreach\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1745\u001b[0m         f(x)\n\u001b[0;32m   1746\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28miter\u001b[39m([])\n\u001b[1;32m-> 1748\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessPartition\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:2297\u001b[0m, in \u001b[0;36mRDD.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   2277\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2279\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   2296\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:2272\u001b[0m, in \u001b[0;36mRDD.sum\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2252\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2253\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[0;32m   2254\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:2025\u001b[0m, in \u001b[0;36mRDD.fold\u001b[1;34m(self, zeroValue, op)\u001b[0m\n\u001b[0;32m   2020\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[0;32m   2022\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[0;32m   2024\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[1;32m-> 2025\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\rdd.py:1814\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1813\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1814\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 114.0 failed 1 times, most recent failure: Lost task 0.0 in stage 114.0 (TID 76) (Priyank executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 15 more\r\n"
     ]
    }
   ],
   "source": [
    "##this is not working\n",
    "\n",
    "import folium\n",
    "\n",
    "# initialize the map\n",
    "fmap = folium.Map(location=[40.738, -73.94], zoom_start=12, tiles=\"CartoDB dark_matter\")\n",
    "\n",
    "# define a function to create a circle marker for each row of data\n",
    "def create_circle_marker(row):\n",
    "    data = row.asDict()\n",
    "    \n",
    "    # create popup text\n",
    "    popup_text = \"{}<br> total entries: {}<br> total exits: {}\".format(\n",
    "        data['station'], int(data['entries']), int(data['exits']))\n",
    "    \n",
    "    \n",
    "    # create circle marker\n",
    "    folium.CircleMarker(\n",
    "        location=(data['latitude'], data['longitude']),\n",
    "        radius= 10,\n",
    "        color=\"#E37222\",\n",
    "        popup=popup_text,\n",
    "        fill=True\n",
    "    ).add_to(fmap)\n",
    "\n",
    "# create a temporary view of the station_day_sum dataframe\n",
    "station_day_sum.createOrReplaceTempView('station_day_sum')\n",
    "\n",
    "# select the rows corresponding to the most recent 5 days\n",
    "most_recent_5 = spark.sql(\"SELECT * FROM station_day_sum WHERE date >= DATE_SUB(CURRENT_DATE(), 5)\")\n",
    "\n",
    "most_recent_5.foreach(create_circle_marker)\n",
    "\n",
    "# display the map\n",
    "fmap.save(\"part_1.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d81684-554b-46d1-8a0d-f553614bbeeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
