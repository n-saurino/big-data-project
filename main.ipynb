{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip\n",
    "# !pip install tqdm\n",
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c960ad5b",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "import findspark\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af63a460",
   "metadata": {},
   "source": [
    "### Dataset links and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=15000000', 'filename': 'arrest.json' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=15000000', 'filename': 'summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/i55r-43gk.json?$limit=15000000', 'filename': 'turnstile.json' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=10000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0596959",
   "metadata": {},
   "source": [
    "### Dataset: Downloading handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "        if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49ab5e10",
   "metadata": {},
   "source": [
    "### Dataset: Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffeb78f4",
   "metadata": {},
   "source": [
    "### Providing Apache Spark backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf9e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/opt/homebrew/Cellar/apache-spark/3.4.0/libexec')\n",
    "findspark.find()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27077460",
   "metadata": {},
   "source": [
    "### Creating spark session using SparkSession builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark session initialization\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[5]\")\\\n",
    "    .appName(\"main\")\\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 50)\\\n",
    "    .config(\"spark.driver.memory\", '4g')\\\n",
    "    .config(\"spark.executor.instances\", 5)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10296165",
   "metadata": {},
   "source": [
    "### Initializing spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "arrest_df = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True).repartition(5)\n",
    "complaint_df = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True).repartition(5)\n",
    "summons_df = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True).repartition(5)\n",
    "turnstile_df = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True).repartition(5)\n",
    "subway_df = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True).repartition(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad5133ca",
   "metadata": {},
   "source": [
    "### Dataset: Analysis & Cleansing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c14ca1f7",
   "metadata": {},
   "source": [
    "#### 1. Subway and Turnstile dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20abbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize udf for station id in subway and turnstile\n",
    "def normalize(unit):\n",
    "    type, *val = list(unit)\n",
    "    return type + str(int(''.join(val)))\n",
    "\n",
    "nUdf = F.udf(normalize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9caf462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize subway GTFS Stop ID\n",
    "subway_df = subway_df.withColumn('GTFS Stop ID', nUdf(F.col('GTFS Stop ID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize turnstile unit\n",
    "turnstile_df = turnstile_df.withColumn('unit', nUdf(F.col('unit')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e7fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting subways stations in turnstile and subway dataset\n",
    "s_list = subway_df.select(F.col('GTFS Stop ID')).distinct().toPandas().values.flatten()\n",
    "t_list = turnstile_df.select(F.col('unit')).distinct().toPandas().values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing common subway stations\n",
    "ts_intersect = [value for value in t_list if value in s_list]\n",
    "ts_intersect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "125ce2c0",
   "metadata": {},
   "source": [
    "#### 2. Arrest dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65367ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_df = arrest_df.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")\n",
    "arrest_df = arrest_df.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", )\\\n",
    "    .filter(F.col('latitude').isNotNull()\\\n",
    "        & F.col('longitude').isNotNull())\\\n",
    "    .withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\"))\\\n",
    "    .withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))\n",
    "arrest_df.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9bc0f14",
   "metadata": {},
   "source": [
    "### Dataset: Consolidating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ce4b4db",
   "metadata": {},
   "source": [
    "1. Combine Subway to Turnstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb822e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = turnstile_df.groupBy(F.col('unit'))\\\n",
    "    .agg({ 'entries': 'sum', 'exits': 'sum'})\\\n",
    "        .select(F.col('unit'),\\\n",
    "            F.col('sum(entries)').alias('entries'),\\\n",
    "            F.col('sum(exits)').alias('exits'))\n",
    "gt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the total exits and entries for each station\n",
    "st_df = subway_df.join(gt_df, F.col('GTFS Stop ID') == F.col('unit'))\\\n",
    "    .select(F.col('GTFS Stop ID').alias('id'),\\\n",
    "        F.col('Line').alias('line'),\\\n",
    "        F.col('Stop Name').alias('stop_name'),\\\n",
    "        F.col('Borough').alias('borough'),\\\n",
    "        F.col('GTFS Latitude').alias('lat'),\\\n",
    "        F.col('GTFS Longitude').alias('long'),\\\n",
    "        F.col('North Direction Label').alias('n_label'),\\\n",
    "        F.col('South Direction Label').alias('s_label'),\\\n",
    "        F.col('entries'),\\\n",
    "        F.col('exits'))\n",
    "st_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff1c8c5b",
   "metadata": {},
   "source": [
    "2. Combine Subway to Arrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f81071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinMeters(slat, slong, dlat, dlong):\n",
    "    srs = (slat, slong)\n",
    "    dst = (dlat, dlong)\n",
    "    print(type(srs[0]),type(srs[1]))\n",
    "    print(srs[0],srs[1])\n",
    "    print(type(dst[0]),type(dst[1]))\n",
    "    print(dst[0],dst[1])\n",
    "    distance = float(haversine(srs, dst,unit=Unit.METERS))\n",
    "    print(distance)\n",
    "    return bool(distance < 402)\n",
    "    \n",
    "withinMetersUdf = F.udf(withinMeters, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7019de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df = subway_df.join(arrest_df, withinMetersUdf(F.col('GTFS Latitude'), F.col('GTFS Longitude'), F.col('latitude'), F.col('longitude')), 'cross')\\\n",
    "    .drop(F.col('latitude'))\\\n",
    "    .drop(F.col('longitude'))\n",
    "sa_df = sa_df.na.drop(subset=[\"Stop Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a84d368",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_count = sa_df.groupBy(F.col('Stop Name')).count()\n",
    "arrest_count.printSchema()\n",
    "arrest_count.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1156034a",
   "metadata": {},
   "source": [
    "3. Combine Subway to Criminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb58d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7f13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca3a1294",
   "metadata": {},
   "source": [
    "4. Combine Subway to Summons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a80a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c93f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92e50f3e",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa191a17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
