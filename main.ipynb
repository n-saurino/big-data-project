{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip\n",
    "# !pip install tqdm\n",
    "# !pip install dask\n",
    "# !pip install apache-sedona\n",
    "# !pip install shapely\n",
    "# !pip install geopandas\n",
    "# !pip install leafmap\n",
    "# !pip install openpyxl\n",
    "# !pip install --upgrade ipyleaflet\n",
    "# !jupyter labextension install jupyter-leaflet\n",
    "# !jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask\n",
    "#!conda install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce98286-fb6b-4af9-994f-689a6f13d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.register import SedonaRegistrator  \n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.types as types\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import leafmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0745851c-98cb-468f-8422-527aa52bcb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.20.174.116:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>main</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1aeda4cb8e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''conf = pyspark.SparkConf()\\\n",
    "    .setMaster(\"local[7]\")\\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .set(\"spark.eventLog.dir\", \"./logs\")\\\n",
    "    .set(\"spark.eventLog.gcMetrics.youngGenerationGarbageCollectors\", \"true\")\\\n",
    "    .set(\"spark.executor.heartbeatInterval\",\"3600s\")\\\n",
    "    .set(\"spark.network.timeout\",\"3601s\")\\\n",
    "    .set(\"spark.sql.repl.eagerEval.enabled\", False)\n",
    "\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel('ERROR')\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark\n",
    "'''\n",
    "\n",
    "# spark session initialization\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[5]\")\\\n",
    "    .appName(\"main\")\\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 50)\\\n",
    "    .config(\"spark.driver.memory\", '8g')\\\n",
    "    .config(\"spark.executor.instances\", 4)\\\n",
    "    .config(\"spark.executor.cores\", 5)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4b68d9-800f-400e-ae44-196e39a7680d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "# historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=10', 'filename': 'lil_arrest.json' }\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.csv?$limit=15000000', 'filename': 'arrest.csv' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=10', 'filename': 'lil_summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/i55r-43gk.json?$limit=15000000', 'filename': 'turnstile.json' }\n",
    "#subway_loc = { 'url': 'https://data.ny.gov/resource/i9wp-a4ja.json?$limit=15000000', 'filename': 'subway.json' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=15000000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "         if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)\n",
    "complaint_rdd = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True)\n",
    "summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)\n",
    "turnstile_df = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True).repartition(5)\n",
    "subway_rdd = spark.read.json(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d239c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "turnstile_df = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2166298f-6ea3-401b-bcbd-603cdd65b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_rdd = spark.read.json(os.path.join(data_dir, subway_loc['filename']), multiLine=True)\n",
    "#subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)\n",
    "# subway_df = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True).repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d9550b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: The number of columns doesn't match.\nOld column names (19): Station ID, Complex ID, GTFS Stop ID, Division, Line, Stop Name, Borough, Daytime Routes, Structure, GTFS Latitude, GTFS Longitude, North Direction Label, South Direction Label, ADA, ADA Direction Notes, ADA NB, ADA SB, Capital Outage NB, Capital Outage SB\nNew column names (8): Station ID, Complex ID, GTFS Stop ID, Division, Line, Stop Name, Borough, Daytime",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m subway_DF \u001b[38;5;241m=\u001b[39m \u001b[43msubway_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStation ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mComplex ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGTFS Stop ID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDivision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mStop Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBorough\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDaytime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\sql\\dataframe.py:5024\u001b[0m, in \u001b[0;36mDataFrame.toDF\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   4993\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtoDF\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4994\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` that with new specified column names\u001b[39;00m\n\u001b[0;32m   4995\u001b[0m \n\u001b[0;32m   4996\u001b[0m \u001b[38;5;124;03m    .. versionchanged:: 3.4.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5022\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[0;32m   5023\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5024\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoDF\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jseq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: The number of columns doesn't match.\nOld column names (19): Station ID, Complex ID, GTFS Stop ID, Division, Line, Stop Name, Borough, Daytime Routes, Structure, GTFS Latitude, GTFS Longitude, North Direction Label, South Direction Label, ADA, ADA Direction Notes, ADA NB, ADA SB, Capital Outage NB, Capital Outage SB\nNew column names (8): Station ID, Complex ID, GTFS Stop ID, Division, Line, Stop Name, Borough, Daytime"
     ]
    }
   ],
   "source": [
    "subway_DF = subway_rdd.toDF(\"Station ID\", \"Complex ID\", \"GTFS Stop ID\", \"Division\", \"Line\", \"Stop Name\", \"Borough\", \"Daytime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a412d-5a3b-45cd-a0f5-fa61a9c6c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b582307-5a6d-4945-b59a-c2ac63d59c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"C:\\\\Users\\\\Nigel\\\\Github\\\\big-data-project\\\\data\\\\arrest.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "arrest_DF = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(arrest_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62079c-c271-4a3f-9c36-88b38171f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8114eb-f461-4336-a3fb-c3bf3aaea18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96698941-1430-4540-b727-3dd24d4230df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d731d-0181-4fb7-a41b-6ee0121047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize udf for station id in subway and turnstile\n",
    "def normalize(unit):\n",
    "    type, *val = list(unit)\n",
    "    return type + str(int(''.join(val)))\n",
    "\n",
    "nUdf = F.udf(normalize, types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b464c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subway_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# normalize subway GTFS Stop ID\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m subway_df \u001b[38;5;241m=\u001b[39m \u001b[43msubway_df\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGTFS Stop ID\u001b[39m\u001b[38;5;124m'\u001b[39m, nUdf(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGTFS Stop ID\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'subway_df' is not defined"
     ]
    }
   ],
   "source": [
    "# normalize subway GTFS Stop ID\n",
    "subway_df = subway_df.withColumn('GTFS Stop ID', nUdf(F.col('GTFS Stop ID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb288f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize turnstile unit\n",
    "turnstile_df = turnstile_df.withColumn('unit', nUdf(F.col('unit')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a471fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting subways stations in turnstile and subway dataset\n",
    "s_list = subway_df.select(F.col('GTFS Stop ID')).distinct().toPandas().values.flatten()\n",
    "t_list = turnstile_df.select(F.col('unit')).distinct().toPandas().values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e63fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing common subway stations\n",
    "ts_intersect = [value for value in t_list if value in s_list]\n",
    "ts_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3287a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_df = turnstile_df.groupBy(F.col('unit'))\\\n",
    "    .agg({ 'entries': 'sum', 'exits': 'sum'})\\\n",
    "        .select(F.col('unit'),\\\n",
    "            F.col('sum(entries)').alias('entries'),\\\n",
    "            F.col('sum(exits)').alias('exits'))\n",
    "gt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28d9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the total exits and entries for each station\n",
    "st_df = subway_df.join(gt_df, F.col('GTFS Stop ID') == F.col('unit'))\\\n",
    "    .select(F.col('GTFS Stop ID').alias('id'),\\\n",
    "        F.col('Line').alias('line'),\\\n",
    "        F.col('Stop Name').alias('stop_name'),\\\n",
    "        F.col('Borough').alias('borough'),\\\n",
    "        F.col('GTFS Latitude').alias('lat'),\\\n",
    "        F.col('GTFS Longitude').alias('long'),\\\n",
    "        F.col('North Direction Label').alias('n_label'),\\\n",
    "        F.col('South Direction Label').alias('s_label'),\\\n",
    "        F.col('entries'),\\\n",
    "        F.col('exits'))\n",
    "st_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29ff0-4812-4fd5-9e3d-267f51352243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_DF = subway_rdd.toDF(\"ada\", \"ada_notes\", \"corner\", \"division\", \"east_west_street\", \"entrance_georeference\", \"entrance_latitude\", \"entrance_location\", \"entrance_longitude\", \"entrance_type\", \"entry\", \"exit_only\", \"free_crossover\", \"line\", \"north_south_street\", \"route1\", \"route10\", \"route11\", \"route2\", \"route3\", \"route4\", \"route5\", \"route6\", \"route7\", \"route8\", \"route9\", \"staff_hours\", \"staffing\", \"station_georeference\", \"station_latitude\", \"station_location\", \"station_longitude\", \"station_name\", \"vending\")\n",
    "subway_DF = subway_rdd.toDF(\"Station ID\", \"Complex ID\", \"GTFS Stop ID\", \"Division\", \"Line\", \"Stop Name\", \"Borough\", \"Daytime Routes\", \"Structure\", \"GTFS Latitude\", \"GTFS Longitude\", \"North Direction Label\", \"South Direction Label\", \"ADA\", \"ADA Direction Notes\", \"ADA NB\", \"ADA SB\", \"Capital Outage NB\", \"Capital Outage SB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1fe1fa-8c11-4996-b464-d3187618b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_DF = subway_DF.select(\"station_name\",\"station_latitude\", \"station_longitude\", \"station_location\", \"station_georeference\")\n",
    "subway_DF = subway_DF.select('Station ID', 'Complex ID', 'GTFS Stop ID', 'Stop Name', 'Borough', 'GTFS Latitude','GTFS Longitude')\\\n",
    "                     .filter(F.col('GTFS Latitude').isNotNull() & F.col('GTFS Longitude').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b63eb5-eb6f-48d3-a646-1c50e101f9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5997b50-94f7-4db6-8bdc-85898cd68bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac8544-d683-4905-8e60-c3e733e64052",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b94f75-2417-4ee8-8686-6e79905352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSIDER DROPPING SUBWAY STOPS WITH DUPLICATE NAMES?\n",
    "# subway_DF.dropDuplicates(subset=[\"Stop Name\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef085fa-0649-43ef-a13d-1fc7d82213ce",
   "metadata": {},
   "source": [
    "Adapting the Haversine function from Homework 2. Setting it to check distance in meters between lat/lon pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775bd688-9fb4-405d-858b-faec2a39910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinMeters(slat, slong, dlat, dlong):\n",
    "    srs = (slat, slong)\n",
    "    dst = (dlat, dlong)\n",
    "    # print(type(srs[0]),type(srs[1]))\n",
    "    # print(srs[0],srs[1])\n",
    "    # print(type(dst[0]),type(dst[1]))\n",
    "    # print(dst[0],dst[1])\n",
    "    distance = float(haversine(srs, dst,unit=Unit.METERS))\n",
    "    # print(distance)\n",
    "    return bool(distance < 402)\n",
    "\n",
    "    \n",
    "withinMetersUdf = F.udf(withinMeters, types.BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00a197-ae7f-4c63-b895-76e3a7f60536",
   "metadata": {},
   "source": [
    "Tried to get the join to work on the full arrest data set but failed and have commented it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96a13a-bfd8-4c86-b554-6ab4c91bfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_DF = arrest_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedde2e-acaf-46b0-b264-8ff9738b8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_DF = arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", ).filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572841d-fa73-4cec-99b4-cd7a5da9ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_DF.crossJoin(arrest_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2540216-3e5a-4558-becc-60c8bd0aaa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0bfa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1386965-df8f-46aa-b0ab-be66a6df21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_arrest_DF.withColumn(\"haversine\", withinMetersUdf(F.col(\"GTFS Latitude\"), F.col(\"GTFS Longitude\"), F.col(\"latitude\"), F.col(\"longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603d037-ba13-4693-8d60-60266ec346de",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8625a2b3-ef8d-44f8-83d1-62ba785bce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f7a0b-8d57-4e0e-bc6f-9f80aaf4b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF = subway_arrest_DF.filter(F.col('haversine') == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a69c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee72ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9576a8d5-e7be-4ca6-98fb-f6211139c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF = subway_arrest_DF.select('Stop Name', 'GTFS Latitude', 'GTFS Longitude',\\\n",
    "    'Borough', 'haversine')\\\n",
    "    .where(subway_arrest_DF.haversine == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c53683d-3103-4315-9448-c084b4b2c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF.groupBy(F.col('Stop Name')).agg(F.count(\"*\").alias(\"Count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4000be-2f30-4b70-ae91-9de85b0810cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce390a10-6d13-4b06-8468-f6cc0a14de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_arrest_DF = subway_DF.join(arrest_DF, withinMetersUdf('GTFS Latitude', 'GTFS Longitude', 'latitude', 'longitude'), 'cross')\\\n",
    "    # .drop(F.col('latitude'))\\\n",
    "    # .drop(F.col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26f802-ec34-4ae7-9c41-f34c08667daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f7b8c-773f-485b-9f82-dac20090e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_count = subway_arrest_DF.groupBy(F.col('Stop Name')).count()\n",
    "# arrest_count = arrest_count.dropDuplicates(subset=[\"Stop Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577651bf-7134-4bdc-9d78-2eeccc680e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c0c78-12dc-422b-a189-76b579506748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_count.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f9d6a-c5bc-4c2b-b150-f69055226a78",
   "metadata": {},
   "source": [
    "Building the smaller arrest dataset (10 rows) and trying to join with the subway data using the haversine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d1f21-ef7c-42e4-86fc-5b80659d33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF = lil_arrest_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35367b0b-8769-4476-a461-0dcafedcbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF = lil_arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", ).filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5ae1e-11b0-4ebc-99f1-7bc311c83cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411b304-f06c-40d7-a43e-7e4b67bd183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ac3fa-b9b5-4202-8524-71253bfbf131",
   "metadata": {},
   "source": [
    "Joining the subway dataframe to the smaller arrest data set unsuccessfully. I tried this with just the strings for the column names and with the F.col() structure but didn't have any success either way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d7306-8b04-4bb4-8c4b-4810efb10063",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_DF.join(lil_arrest_DF, withinMetersUdf(F.col('GTFS Latitude'), F.col('GTFS Longitude'), F.col('latitude'), F.col('longitude')), 'cross')\\\n",
    "    .drop(F.col('latitude'))\\\n",
    "    .drop(F.col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8b69d-9103-4634-b6b7-c8626f84d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute haversine after doing a general crossjoin and create a new column with T/F\n",
    "# Then filter on new column (T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536aa047-7a35-47df-bfc5-8da24caa1bac",
   "metadata": {},
   "source": [
    "The error occurs when trying to execute the pipeline graph with the show() below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427eeaa-d965-4265-81d1-b27ea3542217",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_arrest_DF.na.drop(subset=[\"Stop Name\"])\n",
    "lil_arrest_count = subway_arrest_DF.groupBy(F.col('Stop Name')).count()\n",
    "lil_arrest_count.printSchema()\n",
    "lil_arrest_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587f080-b19d-465d-b3df-a9ce6726989e",
   "metadata": {},
   "source": [
    "Testing the haversine function locally and it works so it seems like there's an issue passing the columns? I checked the hw2 submission and it seems fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc1c11-aa59-45ca-92af-e8d0480ab715",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = withinMeters(40.799008797000056, -73.95240854099995, 40.816391847000034, -73.89529641399997)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f6f94-b2ea-4851-be60-f56ced80a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "summons_DF = summons_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"boro\", \"geocoded_column\", \"jurisdiction_code\", \"latitude\", \"law_description\", \"law_section_number\", \"longitude\", \"offense_description\", \"precinct_of_occur\", \"race\", \"sex\", \"summons_category_type\", \"summons_date\", \"summons_key\", \"x_coordinate_cd\", \"y_coordinate_cd\")\\\n",
    "                        .select(\"boro\",\"latitude\", \"longitude\",\"offense_description\",\"summons_category_type\", \"summons_date\", \"summons_key\").filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a060694-f1db-4704-bfde-805fd7ba868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summons_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae800405-d12a-44f9-905a-d76f51f11afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_DF = lil_summons_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"boro\", \"geocoded_column\", \"jurisdiction_code\", \"latitude\", \"law_description\", \"law_section_number\", \"longitude\", \"offense_description\", \"precinct_of_occur\", \"race\", \"sex\", \"summons_category_type\", \"summons_date\", \"summons_key\", \"x_coordinate_cd\", \"y_coordinate_cd\")\\\n",
    "                        .select(\"boro\",\"latitude\", \"longitude\",\"offense_description\",\"summons_category_type\", \"summons_date\", \"summons_key\").filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a8956-0f2d-4efb-83e1-84bffbf4e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_DF.show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228a7ec-fe46-4383-9538-f23d7c0f386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88618f9-d013-4891-98af-213e75611fc0",
   "metadata": {},
   "source": [
    "Figuring out the map plotting for the subway locations in geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5fbdb-1761-4529-bbf2-98354af248bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1798ac54-d193-40f2-9334-c5bb144ceba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subway_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_DF \u001b[38;5;241m=\u001b[39m \u001b[43msubway_DF\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrests\u001b[39m\u001b[38;5;124m\"\u001b[39m, ((F\u001b[38;5;241m.\u001b[39mrand() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80000\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m20000\u001b[39m)\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'subway_DF' is not defined"
     ]
    }
   ],
   "source": [
    "test_DF = subway_DF.withColumn(\"arrests\", ((F.rand() * 80000) + 20000).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75713dde-c93a-42ec-b52d-e55d90cb378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97806b08-851f-430f-a4cc-9aeea983673b",
   "metadata": {},
   "source": [
    "Once we have the dataset bounded at <500 rows (a row per station), we should convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f077a2-f482-48a9-9b5f-1e735c51e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF = test_DF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568f676-0bc7-48f8-8ce3-aa3a7f64a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b2b48-462c-48f4-aa1b-03268860e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF['geometry'] = [Point(xy) for xy in zip(test_DF['GTFS Longitude'],test_DF['GTFS Latitude'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b7715-5fcc-473b-82da-ab08ef4516f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF[\"arrest_pct_total\"] = (test_DF[\"arrests\"] / test_DF[\"arrests\"].sum()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988d401-9f92-407d-b402-e25b42a81cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF = test_DF.sort_values(by='arrest_pct_total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff71af3-b18f-47cf-8663-26822ed9fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de97a63-ec6d-4a88-b728-02b952d95ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF[\"Borough\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714db4b6-c745-449d-83c6-965a2c74c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_map = gpd.read_file(r'C:\\Users\\Nigel\\Github\\big-data-project\\data\\nynta2020_23a\\nynta2020.shp')\n",
    "nta_map.to_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5499f-5653-4d5d-b5ae-97bcab192bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geo_DF = gpd.GeoDataFrame(test_DF, crs=4326, geometry = test_DF.geometry)\n",
    "# Just to be extra sure\n",
    "test_geo_DF.to_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31b9d3-f60c-4955-b181-83fbf67c6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "nta_map.boundary.plot(ax=ax, edgecolor='k');\n",
    "test_geo_DF[test_geo_DF.Borough == 'M']\\\n",
    "            .head(10)\\\n",
    "            .plot(column='Stop Name', ax=ax, legend=True, marker='.',\\\n",
    "            markersize=test_geo_DF.arrest_pct_total.astype('float') * 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53337ace-4461-4201-8c0a-bceb340306b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_test_geo_DF = test_geo_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9e961-519b-4ccc-a941-a4ebe3a7953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_demos = pandas.read_excel('https://www1.nyc.gov/assets/planning/download/office/planning-level/nyc-population/acs/demo_2019_acs5yr_nta.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55b7f3-97c1-4830-b15f-394b634fb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_df = nta_map.merge(nta_demos, how='left', left_on='NTA2020', right_on='GeoID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9775b8-3803-4cd8-a0d8-69ba24daac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_test_geo_DF.to_file(\"test_geo.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5a74d-f9d0-48e4-b04f-aacb05205450",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(center=(40,-100),zoom=4)\n",
    "m.add_gdf(nta_df, layer_name='2020 NTA Demographic Information', info_mode='on_click')\n",
    "m.add_point_layer(filename=r'C:\\Users\\Nigel\\Github\\big-data-project\\test_geo.json', popup=['Stop Name', 'arrests', 'arrest_pct_total'], layer_name=\"Stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563e154-e29c-4711-8bf1-9bc7407765c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd11008-250b-43d3-a12a-16f9f3a19590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
