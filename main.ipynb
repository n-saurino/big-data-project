{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip\n",
    "# !pip install tqdm\n",
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c960ad5b",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, BooleanType\n",
    "import findspark\n",
    "from haversine import haversine, Unit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af63a460",
   "metadata": {},
   "source": [
    "### Dataset links and filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=15000000', 'filename': 'arrest.json' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=15000000', 'filename': 'summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/i55r-43gk.json?$limit=15000000', 'filename': 'turnstile.json' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=10000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0596959",
   "metadata": {},
   "source": [
    "### Dataset: Downloading handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "        if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49ab5e10",
   "metadata": {},
   "source": [
    "### Dataset: Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffeb78f4",
   "metadata": {},
   "source": [
    "### Providing Apache Spark backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfcf9e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/homebrew/Cellar/apache-spark/3.4.0/libexec'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findspark.init('/opt/homebrew/Cellar/apache-spark/3.4.0/libexec')\n",
    "findspark.find()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27077460",
   "metadata": {},
   "source": [
    "### Creating spark session using SparkSession builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e4059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/07 03:32:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://devarshs-mbp.fios-router.home:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[5]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>main</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11f362340>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session initialization\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[5]\")\\\n",
    "    .appName(\"main\")\\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 50)\\\n",
    "    .config(\"spark.driver.memory\", '8g')\\\n",
    "    .config(\"spark.executor.instances\", 4)\\\n",
    "    .config(\"spark.executor.cores\", 5)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36513cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.executor.cores\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10296165",
   "metadata": {},
   "source": [
    "### Initializing spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/07 03:33:01 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dataframes\n",
    "arrest_df = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True).repartition(5)\n",
    "complaint_df = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True).repartition(5)\n",
    "summons_df = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True).repartition(5)\n",
    "turnstile_df = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True).repartition(5)\n",
    "subway_df = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True).repartition(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad5133ca",
   "metadata": {},
   "source": [
    "### Dataset: Analysis & Cleansing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c14ca1f7",
   "metadata": {},
   "source": [
    "#### 1. Subway and Turnstile dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a20abbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize udf for station id in subway and turnstile\n",
    "def normalize(unit):\n",
    "    type, *val = list(unit)\n",
    "    return type + str(int(''.join(val)))\n",
    "\n",
    "nUdf = F.udf(normalize, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9caf462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize subway GTFS Stop ID\n",
    "subway_df = subway_df.withColumn('GTFS Stop ID', nUdf(F.col('GTFS Stop ID')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eec408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize turnstile unit\n",
    "turnstile_df = turnstile_df.withColumn('unit', nUdf(F.col('unit')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "070e7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# getting subways stations in turnstile and subway dataset\n",
    "s_list = subway_df.select(F.col('GTFS Stop ID')).distinct().toPandas().values.flatten()\n",
    "t_list = turnstile_df.select(F.col('unit')).distinct().toPandas().values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9adb974f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['R16',\n",
       " 'R13',\n",
       " 'R6',\n",
       " 'R30',\n",
       " 'R8',\n",
       " 'R40',\n",
       " 'R23',\n",
       " 'R45',\n",
       " 'R21',\n",
       " 'R20',\n",
       " 'R43',\n",
       " 'R3',\n",
       " 'R36',\n",
       " 'R15',\n",
       " 'R24',\n",
       " 'R29',\n",
       " 'R33',\n",
       " 'R39',\n",
       " 'R4',\n",
       " 'R41',\n",
       " 'R27',\n",
       " 'R19',\n",
       " 'R5',\n",
       " 'R44',\n",
       " 'R22',\n",
       " 'R34',\n",
       " 'R32',\n",
       " 'R31',\n",
       " 'R42',\n",
       " 'R9',\n",
       " 'R11',\n",
       " 'R1',\n",
       " 'R25',\n",
       " 'R18',\n",
       " 'R14',\n",
       " 'R35',\n",
       " 'R17',\n",
       " 'R28']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing common subway stations\n",
    "ts_intersect = [value for value in t_list if value in s_list]\n",
    "ts_intersect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9bc0f14",
   "metadata": {},
   "source": [
    "### Dataset: Consolidating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ce4b4db",
   "metadata": {},
   "source": [
    "1. Combine Subway to Turnstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "beb822e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------------+\n",
      "|unit|         entries|           exits|\n",
      "+----+----------------+----------------+\n",
      "|R159| 9.7658670459E10| 3.6752142042E10|\n",
      "|R143|6.16250900125E11|7.75773326471E11|\n",
      "|R469| 1.0314052482E10|    9.79342886E8|\n",
      "| R16| 7.9053401996E10| 9.4031973197E10|\n",
      "|R167| 1.6874102816E11|1.96763060056E11|\n",
      "|R100| 3.8355082149E10| 4.2813609167E10|\n",
      "|R177|2.21718982614E11|1.91598461153E11|\n",
      "|R290| 7.7285465068E10| 9.9919615555E10|\n",
      "| R13| 9.9819375916E10|1.70757933631E11|\n",
      "| R37| 2.8851327936E11| 1.8206496699E10|\n",
      "|R384|  2.283725805E10| 2.2146211814E10|\n",
      "|R432| 1.5507768821E10|   7.016570014E9|\n",
      "| R80|2.69169144454E11| 1.5734215062E11|\n",
      "|R541| 2.0813861412E10| 1.6886431668E10|\n",
      "|R412|   9.997534418E9|    3.88309983E9|\n",
      "|R430|   9.052436119E9|  1.126470821E10|\n",
      "|R294| 4.0256229752E10| 2.4555010956E10|\n",
      "| R95| 4.8301616614E10| 4.9211442273E10|\n",
      "|  R6| 7.1331940846E10| 2.6958063183E10|\n",
      "|R329| 1.1117120569E10|   5.684716085E9|\n",
      "+----+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "gt_df = turnstile_df.groupBy(F.col('unit'))\\\n",
    "    .agg({ 'entries': 'sum', 'exits': 'sum'})\\\n",
    "        .select(F.col('unit'),\\\n",
    "            F.col('sum(entries)').alias('entries'),\\\n",
    "            F.col('sum(exits)').alias('exits'))\n",
    "gt_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffe1cb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----------------+-------+---------+----------+--------------------+--------------------+-----------------+-----------------+\n",
      "| id|               line|        stop_name|borough|      lat|      long|             n_label|             s_label|          entries|            exits|\n",
      "+---+-------------------+-----------------+-------+---------+----------+--------------------+--------------------+-----------------+-----------------+\n",
      "|R16|Broadway - Brighton|   Times Sq-42 St|      M|40.754672|-73.986754|     Uptown & Queens| Downtown & Brooklyn|  7.9053401996E10|  9.4031973197E10|\n",
      "|R13|            Astoria|       5 Av/59 St|      M|40.764811|-73.973347|              Queens| Downtown & Brooklyn|  9.9819375916E10| 1.70757933631E11|\n",
      "| R6|            Astoria|            36 Av|      Q|40.756804|-73.929575|Astoria - Ditmars...|           Manhattan|  7.1331940846E10|  2.6958063183E10|\n",
      "|R30|Broadway - Brighton|        DeKalb Av|     Bk|40.690635|-73.981824|           Manhattan|Coney Island - Ba...|1.625048003534E12|1.334749653979E12|\n",
      "| R8|            Astoria|39 Av-Dutch Kills|      Q|40.752882|-73.932755|Astoria - Ditmars...|           Manhattan|  2.7676840763E10|  2.6753306073E10|\n",
      "|R40|             4th Av|            53 St|     Bk|40.645069|-74.014034|           Manhattan|   Bay Ridge - 95 St| 1.18343499418E11|  1.3138969255E10|\n",
      "|R23|           Broadway|         Canal St|      M|40.719527|-74.001775|     Uptown & Queens| Downtown & Brooklyn|3.034008171273E12|4.004890364733E12|\n",
      "|R45|             4th Av|  Bay Ridge-95 St|     Bk|40.616622|-74.030876|           Manhattan|                null| 1.45718172033E11|  1.1424377247E11|\n",
      "|R21|Broadway - Brighton|         8 St-NYU|      M|40.730328|-73.992629|     Uptown & Queens| Downtown & Brooklyn|1.067086537847E12| 8.97730531215E11|\n",
      "|R20|Broadway - Brighton|   14 St-Union Sq|      M|40.735736|-73.990568|     Uptown & Queens| Downtown & Brooklyn| 3.52359562127E11| 3.20595332347E11|\n",
      "|R43|             4th Av|            77 St|     Bk|40.629742| -74.02551|           Manhattan|               95 St| 7.74701516828E11| 1.27076914353E11|\n",
      "| R3|            Astoria|     Astoria Blvd|      Q|40.770258|-73.917843|        Ditmars Blvd|           Manhattan|  1.0233724436E10|    6.300884972E9|\n",
      "|R36|             4th Av|            36 St|     Bk|40.655144|-74.003549|           Manhattan|Coney Island - Ba...|    9.851525611E9|  1.4600105563E10|\n",
      "|R15|Broadway - Brighton|            49 St|      M|40.759901|-73.984139|     Uptown & Queens| Downtown & Brooklyn| 1.71540057484E11| 1.83989511603E11|\n",
      "|R24|           Broadway|        City Hall|      M|40.713282|-74.006978|     Uptown & Queens| Downtown & Brooklyn| 2.77143996739E11|  1.3558776869E11|\n",
      "|R29|           Broadway| Jay St-MetroTech|     Bk| 40.69218|-73.985942|           Manhattan|   Bay Ridge - 95 St| 6.20282148955E11| 4.43236158889E11|\n",
      "|R33|             4th Av|        4 Av-9 St|     Bk|40.670847|-73.988302|           Manhattan|   Bay Ridge - 95 St|3.697545906933E12|5.445914142777E12|\n",
      "|R39|             4th Av|            45 St|     Bk|40.648939|-74.010006|           Manhattan|   Bay Ridge - 95 St|  1.0134315912E10|    1.548481925E9|\n",
      "| R4|            Astoria|            30 Av|      Q|40.766779|-73.921479|Astoria - Ditmars...|           Manhattan|  3.1864562633E10|  3.1061851564E10|\n",
      "|R41|             4th Av|            59 St|     Bk|40.641362|-74.017881|           Manhattan|Coney Island - Ba...| 2.32108548046E11|  9.8903419027E10|\n",
      "+---+-------------------+-----------------+-------+---------+----------+--------------------+--------------------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# combine the total exits and entries for each station\n",
    "st_df = subway_df.join(gt_df, F.col('GTFS Stop ID') == F.col('unit'))\\\n",
    "    .select(F.col('GTFS Stop ID').alias('id'),\\\n",
    "        F.col('Line').alias('line'),\\\n",
    "        F.col('Stop Name').alias('stop_name'),\\\n",
    "        F.col('Borough').alias('borough'),\\\n",
    "        F.col('GTFS Latitude').alias('lat'),\\\n",
    "        F.col('GTFS Longitude').alias('long'),\\\n",
    "        F.col('North Direction Label').alias('n_label'),\\\n",
    "        F.col('South Direction Label').alias('s_label'),\\\n",
    "        F.col('entries'),\\\n",
    "        F.col('exits'))\n",
    "st_df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff1c8c5b",
   "metadata": {},
   "source": [
    "2. Combine Subway to Arrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8f81071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinRange(slat, slong, dlat, dlong):\n",
    "    srs = (slat, slong)\n",
    "    dst = (dlat, dlong)\n",
    "    distance = float(haversine(srs, dst,unit=Unit.MILES))\n",
    "    return distance < 0.1\n",
    "    \n",
    "withinRangeUdf = F.udf(withinRange, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7019de02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':@computed_region_92fq_4b7q',\n",
       " ':@computed_region_efsh_h5xi',\n",
       " ':@computed_region_f5dn_yrer',\n",
       " ':@computed_region_sbqj_enih',\n",
       " ':@computed_region_yeji_bk3q',\n",
       " 'age_group',\n",
       " 'arrest_boro',\n",
       " 'arrest_date',\n",
       " 'arrest_key',\n",
       " 'arrest_precinct',\n",
       " 'jurisdiction_code',\n",
       " 'ky_cd',\n",
       " 'latitude',\n",
       " 'law_cat_cd',\n",
       " 'law_code',\n",
       " 'lon_lat',\n",
       " 'longitude',\n",
       " 'ofns_desc',\n",
       " 'pd_cd',\n",
       " 'pd_desc',\n",
       " 'perp_race',\n",
       " 'perp_sex',\n",
       " 'x_coord_cd',\n",
       " 'y_coord_cd']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrest_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a84d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/07 03:39:45 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 40)1]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.NoopFilters.<init>(StructFilters.scala:163)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject$default$4(JacksonParser.scala:444)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:359)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:404)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:359)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4114/0x000000080200c7a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertArray(JacksonParser.scala:526)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:404)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4124/0x000000080200f058.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:561)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4135/0x000000080200b688.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:556)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.$anonfun$readFile$16(JsonDataSource.scala:223)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$$$Lambda$4132/0x000000080200ac28.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.readFile(JsonDataSource.scala:229)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.$anonfun$buildReader$2(JsonFileFormat.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$Lambda$4110/0x0000000802007000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$3388/0x0000000801ef5310.apply(Unknown Source)\n",
      "23/05/07 03:39:45 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[#78,Executor task launch worker for task 0.0 in stage 33.0 (TID 40),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.NoopFilters.<init>(StructFilters.scala:163)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject$default$4(JacksonParser.scala:444)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:359)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:404)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:359)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4114/0x000000080200c7a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertArray(JacksonParser.scala:526)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:404)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4124/0x000000080200f058.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:561)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4135/0x000000080200b688.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:556)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.$anonfun$readFile$16(JsonDataSource.scala:223)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$$$Lambda$4132/0x000000080200ac28.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.readFile(JsonDataSource.scala:229)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.$anonfun$buildReader$2(JsonFileFormat.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$Lambda$4110/0x0000000802007000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$3388/0x0000000801ef5310.apply(Unknown Source)\n",
      "23/05/07 03:39:45 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 40) (devarshs-mbp.fios-router.home executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.sql.catalyst.NoopFilters.<init>(StructFilters.scala:163)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertObject$default$4(JacksonParser.scala:444)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:360)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeConverter$19$1.applyOrElse(JacksonParser.scala:359)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:404)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeConverter$19(JacksonParser.scala:359)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4114/0x000000080200c7a0.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.org$apache$spark$sql$catalyst$json$JacksonParser$$convertArray(JacksonParser.scala:526)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$$nestedInanonfun$makeStructRootConverter$3$1.applyOrElse(JacksonParser.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parseJsonToken(JacksonParser.scala:404)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$makeStructRootConverter$3(JacksonParser.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4124/0x000000080200f058.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.$anonfun$parse$2(JacksonParser.scala:561)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser$$Lambda$4135/0x000000080200b688.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2786)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:556)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.$anonfun$readFile$16(JsonDataSource.scala:223)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$$$Lambda$4132/0x000000080200ac28.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.MultiLineJsonDataSource$.readFile(JsonDataSource.scala:229)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.$anonfun$buildReader$2(JsonFileFormat.scala:126)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$Lambda$4110/0x0000000802007000.apply(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:155)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:140)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:228)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:290)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:125)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:225)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.$anonfun$prepareShuffleDependency$10(ShuffleExchangeExec.scala:369)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$$Lambda$3388/0x0000000801ef5310.apply(Unknown Source)\n",
      "\n",
      "23/05/07 03:39:45 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job\n",
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 52929)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <unprintable Py4JJavaError object>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:py4j.reflection.TypeUtil.isInstanceOf",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(61, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m arrest_df\u001b[39m.\u001b[39;49mshow()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, \u001b[39m20\u001b[39;49m, vertical))\n\u001b[1;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:171\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 171\u001b[0m     converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39;49mjava_exception)\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m         \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m         \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m         \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:128\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39melif\u001b[39;00m is_instance_of(gw, e, \u001b[39m\"\u001b[39m\u001b[39morg.apache.spark.sql.streaming.StreamingQueryException\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    127\u001b[0m     \u001b[39mreturn\u001b[39;00m StreamingQueryException(origin\u001b[39m=\u001b[39me)\n\u001b[0;32m--> 128\u001b[0m \u001b[39melif\u001b[39;00m is_instance_of(gw, e, \u001b[39m\"\u001b[39;49m\u001b[39morg.apache.spark.sql.execution.QueryExecutionException\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m QueryExecutionException(origin\u001b[39m=\u001b[39me)\n\u001b[1;32m    130\u001b[0m \u001b[39m# Order matters. NumberFormatException inherits IllegalArgumentException.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/java_gateway.py:464\u001b[0m, in \u001b[0;36mis_instance_of\u001b[0;34m(gateway, java_object, java_class)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    462\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mjava_class must be a string, a JavaClass, or a JavaObject\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 464\u001b[0m \u001b[39mreturn\u001b[39;00m gateway\u001b[39m.\u001b[39;49mjvm\u001b[39m.\u001b[39;49mpy4j\u001b[39m.\u001b[39;49mreflection\u001b[39m.\u001b[39;49mTypeUtil\u001b[39m.\u001b[39;49misInstanceOf(\n\u001b[1;32m    465\u001b[0m     param, java_object)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/bdproject/lib/python3.8/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:py4j.reflection.TypeUtil.isInstanceOf"
     ]
    }
   ],
   "source": [
    "arrest_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f835e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_df = subway_df.join(arrest_df, withinRange(F.col('GTFS Latitude'),\\\n",
    "    F.col('GTFS Longitude'),\\\n",
    "    F.col('latitude'),\\\n",
    "    F.col('longitude')), 'cross')\n",
    "sa_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864710f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32fad4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92db101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1156034a",
   "metadata": {},
   "source": [
    "3. Combine Subway to Criminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb58d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7f13d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca3a1294",
   "metadata": {},
   "source": [
    "4. Combine Subway to Summons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a80a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54c93f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92e50f3e",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa191a17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
