{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip\n",
    "# !pip install tqdm\n",
    "# !pip install dask\n",
    "# !pip install apache-sedona\n",
    "# !pip install shapely\n",
    "# !pip install geopandas\n",
    "# !pip install leafmap\n",
    "# !pip install openpyxl\n",
    "!pip install --upgrade ipyleaflet\n",
    "!jupyter labextension install jupyter-leaflet\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask\n",
    "#!conda install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce98286-fb6b-4af9-994f-689a6f13d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.register import SedonaRegistrator  \n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.types as types\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.dates as mdates\n",
    "import leafmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import pyspark\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0745851c-98cb-468f-8422-527aa52bcb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.16.222.8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[7]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1dc35a3fbb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    "    .setMaster(\"local[7]\")\\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .set(\"spark.eventLog.dir\", \"./logs\")\\\n",
    "    .set(\"spark.eventLog.gcMetrics.youngGenerationGarbageCollectors\", \"true\")\\\n",
    "    .set(\"spark.executor.heartbeatInterval\",\"3600s\")\\\n",
    "    .set(\"spark.network.timeout\",\"3601s\")\\\n",
    "    .set(\"spark.sql.repl.eagerEval.enabled\", False)\n",
    "    # .set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\\\n",
    "    # .set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\\\n",
    "    # .set('spark.jars.packages',\n",
    "           # 'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n",
    "           # 'org.datasyslab:geotools-wrapper:1.4.0-28.2')\n",
    "\n",
    "'''\n",
    "sparkSession = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "    getOrCreate()\n",
    "'''\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel('ERROR')\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b68d9-800f-400e-ae44-196e39a7680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "# historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=10', 'filename': 'lil_arrest.json' }\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.csv?$limit=15000000', 'filename': 'arrest.csv' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=10', 'filename': 'lil_summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/i55r-43gk.json?$limit=15000000', 'filename': 'turnstile.json' }\n",
    "#subway_loc = { 'url': 'https://data.ny.gov/resource/i9wp-a4ja.json?$limit=15000000', 'filename': 'subway.json' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=15000000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "         if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "arrest.csv: 1.49GB [03:05, 8.05MB/s]\n",
      "arrest.csv: 1.49GB [03:05, 8.05MB/s]"
     ]
    }
   ],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)\n",
    "complaint_rdd = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True)\n",
    "summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)\n",
    "turnstile_rdd = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True)\n",
    "subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2166298f-6ea3-401b-bcbd-603cdd65b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_rdd = spark.read.json(os.path.join(data_dir, subway_loc['filename']), multiLine=True)\n",
    "subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a412d-5a3b-45cd-a0f5-fa61a9c6c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b582307-5a6d-4945-b59a-c2ac63d59c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[arrest_key: int, arrest_date: timestamp, pd_cd: double, pd_desc: string, ky_cd: double, ofns_desc: string, law_code: string, law_cat_cd: string, arrest_boro: string, arrest_precinct: int, jurisdiction_code: double, age_group: string, perp_sex: string, perp_race: string, x_coord_cd: double, y_coord_cd: double, latitude: double, longitude: double, lon_lat: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_location = \"C:\\\\Users\\\\Nigel\\\\Github\\\\big-data-project\\\\data\\\\arrest.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "arrest_DF = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(arrest_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd62079c-c271-4a3f-9c36-88b38171f1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5498650"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8114eb-f461-4336-a3fb-c3bf3aaea18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96698941-1430-4540-b727-3dd24d4230df",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d731d-0181-4fb7-a41b-6ee0121047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b29ff0-4812-4fd5-9e3d-267f51352243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_DF = subway_rdd.toDF(\"ada\", \"ada_notes\", \"corner\", \"division\", \"east_west_street\", \"entrance_georeference\", \"entrance_latitude\", \"entrance_location\", \"entrance_longitude\", \"entrance_type\", \"entry\", \"exit_only\", \"free_crossover\", \"line\", \"north_south_street\", \"route1\", \"route10\", \"route11\", \"route2\", \"route3\", \"route4\", \"route5\", \"route6\", \"route7\", \"route8\", \"route9\", \"staff_hours\", \"staffing\", \"station_georeference\", \"station_latitude\", \"station_location\", \"station_longitude\", \"station_name\", \"vending\")\n",
    "subway_DF = subway_rdd.toDF(\"Station ID\", \"Complex ID\", \"GTFS Stop ID\", \"Division\", \"Line\", \"Stop Name\", \"Borough\", \"Daytime Routes\", \"Structure\", \"GTFS Latitude\", \"GTFS Longitude\", \"North Direction Label\", \"South Direction Label\", \"ADA\", \"ADA Direction Notes\", \"ADA NB\", \"ADA SB\", \"Capital Outage NB\", \"Capital Outage SB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e1fe1fa-8c11-4996-b464-d3187618b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_DF = subway_DF.select(\"station_name\",\"station_latitude\", \"station_longitude\", \"station_location\", \"station_georeference\")\n",
    "subway_DF = subway_DF.select('Station ID', 'Complex ID', 'GTFS Stop ID', 'Stop Name', 'Borough', 'GTFS Latitude','GTFS Longitude')\\\n",
    "                     .filter(F.col('GTFS Latitude').isNotNull() & F.col('GTFS Longitude').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "43b63eb5-eb6f-48d3-a646-1c50e101f9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+--------------------+-------+-------------+--------------+\n",
      "|Station ID|Complex ID|GTFS Stop ID|           Stop Name|Borough|GTFS Latitude|GTFS Longitude|\n",
      "+----------+----------+------------+--------------------+-------+-------------+--------------+\n",
      "|         1|         1|         R01|Astoria-Ditmars Blvd|      Q|    40.775036|    -73.912034|\n",
      "|         2|         2|         R03|        Astoria Blvd|      Q|    40.770258|    -73.917843|\n",
      "|         3|         3|         R04|               30 Av|      Q|    40.766779|    -73.921479|\n",
      "|         4|         4|         R05|            Broadway|      Q|     40.76182|    -73.925508|\n",
      "|         5|         5|         R06|               36 Av|      Q|    40.756804|    -73.929575|\n",
      "|         6|         6|         R08|   39 Av-Dutch Kills|      Q|    40.752882|    -73.932755|\n",
      "|         7|       613|         R11|  Lexington Av/59 St|      M|     40.76266|    -73.967258|\n",
      "|         8|         8|         R13|          5 Av/59 St|      M|    40.764811|    -73.973347|\n",
      "|         9|         9|         R14|          57 St-7 Av|      M|    40.764664|    -73.980658|\n",
      "|        10|        10|         R15|               49 St|      M|    40.759901|    -73.984139|\n",
      "|        11|       611|         R16|      Times Sq-42 St|      M|    40.754672|    -73.986754|\n",
      "|        12|       607|         R17|     34 St-Herald Sq|      M|    40.749567|     -73.98795|\n",
      "|        13|        13|         R18|               28 St|      M|    40.745494|    -73.988691|\n",
      "|        14|        14|         R19|               23 St|      M|    40.741303|    -73.989344|\n",
      "|        15|       602|         R20|      14 St-Union Sq|      M|    40.735736|    -73.990568|\n",
      "|        16|        16|         R21|            8 St-NYU|      M|    40.730328|    -73.992629|\n",
      "|        17|        17|         R22|           Prince St|      M|    40.724329|    -73.997702|\n",
      "|        18|       623|         R23|            Canal St|      M|    40.719527|    -74.001775|\n",
      "|        19|       623|         Q01|            Canal St|      M|    40.718383|     -74.00046|\n",
      "|        20|        20|         R24|           City Hall|      M|    40.713282|    -74.006978|\n",
      "+----------+----------+------------+--------------------+-------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5997b50-94f7-4db6-8bdc-85898cd68bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station ID: integer (nullable = true)\n",
      " |-- Complex ID: integer (nullable = true)\n",
      " |-- GTFS Stop ID: string (nullable = true)\n",
      " |-- Stop Name: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- GTFS Latitude: double (nullable = true)\n",
      " |-- GTFS Longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac8544-d683-4905-8e60-c3e733e64052",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b94f75-2417-4ee8-8686-6e79905352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSIDER DROPPING SUBWAY STOPS WITH DUPLICATE NAMES?\n",
    "# subway_DF.dropDuplicates(subset=[\"Stop Name\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef085fa-0649-43ef-a13d-1fc7d82213ce",
   "metadata": {},
   "source": [
    "Adapting the Haversine function from Homework 2. Setting it to check distance in meters between lat/lon pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "775bd688-9fb4-405d-858b-faec2a39910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinMeters(slat, slong, dlat, dlong):\n",
    "    srs = (slat, slong)\n",
    "    dst = (dlat, dlong)\n",
    "    # print(type(srs[0]),type(srs[1]))\n",
    "    # print(srs[0],srs[1])\n",
    "    # print(type(dst[0]),type(dst[1]))\n",
    "    # print(dst[0],dst[1])\n",
    "    distance = float(haversine(srs, dst,unit=Unit.METERS))\n",
    "    # print(distance)\n",
    "    return bool(distance < 402)\n",
    "\n",
    "    \n",
    "withinMetersUdf = F.udf(withinMeters, types.BooleanType())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f00a197-ae7f-4c63-b895-76e3a7f60536",
   "metadata": {},
   "source": [
    "Tried to get the join to work on the full arrest data set but failed and have commented it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96a13a-bfd8-4c86-b554-6ab4c91bfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_DF = arrest_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bedde2e-acaf-46b0-b264-8ff9738b8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_DF = arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", ).filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2572841d-fa73-4cec-99b4-cd7a5da9ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_DF.crossJoin(arrest_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2540216-3e5a-4558-becc-60c8bd0aaa33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station ID: integer (nullable = true)\n",
      " |-- Complex ID: integer (nullable = true)\n",
      " |-- GTFS Stop ID: string (nullable = true)\n",
      " |-- Stop Name: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- GTFS Latitude: double (nullable = true)\n",
      " |-- GTFS Longitude: double (nullable = true)\n",
      " |-- arrest_boro: string (nullable = true)\n",
      " |-- arrest_date: timestamp (nullable = true)\n",
      " |-- arrest_key: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1386965-df8f-46aa-b0ab-be66a6df21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_arrest_DF.withColumn(\"haversine\", withinMetersUdf(F.col(\"GTFS Latitude\"), F.col(\"GTFS Longitude\"), F.col(\"latitude\"), F.col(\"longitude\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f603d037-ba13-4693-8d60-60266ec346de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2727329904"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8625a2b3-ef8d-44f8-83d1-62ba785bce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station ID: integer (nullable = true)\n",
      " |-- Complex ID: integer (nullable = true)\n",
      " |-- GTFS Stop ID: string (nullable = true)\n",
      " |-- Stop Name: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- GTFS Latitude: double (nullable = true)\n",
      " |-- GTFS Longitude: double (nullable = true)\n",
      " |-- arrest_boro: string (nullable = true)\n",
      " |-- arrest_date: timestamp (nullable = true)\n",
      " |-- arrest_key: integer (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- haversine: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "968f7a0b-8d57-4e0e-bc6f-9f80aaf4b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF = subway_arrest_DF.filter(F.col('haversine') == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9576a8d5-e7be-4ca6-98fb-f6211139c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_subway_arrest_DF = subway_arrest_DF.select('Stop Name', 'GTFS Latitude', 'GTFS Longitude',\\\n",
    "    'Borough', 'haversine')\\\n",
    "    .where(subway_arrest_DF.haversine == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c53683d-3103-4315-9448-c084b4b2c39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Stop Name: string, Count: bigint]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_subway_arrest_DF.groupBy(F.col('Stop Name')).agg(F.count(\"*\").alias(\"Count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1d4000be-2f30-4b70-ae91-9de85b0810cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o284.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 60.0 failed 1 times, most recent failure: Lost task 4.0 in stage 60.0 (TID 212) (10.16.222.8 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfiltered_subway_arrest_DF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1172\u001b[0m \n\u001b[0;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o284.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 60.0 failed 1 times, most recent failure: Lost task 4.0 in stage 60.0 (TID 212) (10.16.222.8 executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "filtered_subway_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce390a10-6d13-4b06-8468-f6cc0a14de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_arrest_DF = subway_DF.join(arrest_DF, withinMetersUdf('GTFS Latitude', 'GTFS Longitude', 'latitude', 'longitude'), 'cross')\\\n",
    "    # .drop(F.col('latitude'))\\\n",
    "    # .drop(F.col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26f802-ec34-4ae7-9c41-f34c08667daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "862f7b8c-773f-485b-9f82-dac20090e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_count = subway_arrest_DF.groupBy(F.col('Stop Name')).count()\n",
    "# arrest_count = arrest_count.dropDuplicates(subset=[\"Stop Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577651bf-7134-4bdc-9d78-2eeccc680e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c0c78-12dc-422b-a189-76b579506748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arrest_count.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f9d6a-c5bc-4c2b-b150-f69055226a78",
   "metadata": {},
   "source": [
    "Building the smaller arrest dataset (10 rows) and trying to join with the subway data using the haversine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209d1f21-ef7c-42e4-86fc-5b80659d33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF = lil_arrest_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35367b0b-8769-4476-a461-0dcafedcbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF = lil_arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", ).filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5ae1e-11b0-4ebc-99f1-7bc311c83cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a411b304-f06c-40d7-a43e-7e4b67bd183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ac3fa-b9b5-4202-8524-71253bfbf131",
   "metadata": {},
   "source": [
    "Joining the subway dataframe to the smaller arrest data set unsuccessfully. I tried this with just the strings for the column names and with the F.col() structure but didn't have any success either way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52d7306-8b04-4bb4-8c4b-4810efb10063",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_DF.join(lil_arrest_DF, withinMetersUdf(F.col('GTFS Latitude'), F.col('GTFS Longitude'), F.col('latitude'), F.col('longitude')), 'cross')\\\n",
    "    .drop(F.col('latitude'))\\\n",
    "    .drop(F.col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db8b69d-9103-4634-b6b7-c8626f84d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute haversine after doing a general crossjoin and create a new column with T/F\n",
    "# Then filter on new column (T/F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536aa047-7a35-47df-bfc5-8da24caa1bac",
   "metadata": {},
   "source": [
    "The error occurs when trying to execute the pipeline graph with the show() below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3427eeaa-d965-4265-81d1-b27ea3542217",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_arrest_DF = subway_arrest_DF.na.drop(subset=[\"Stop Name\"])\n",
    "lil_arrest_count = subway_arrest_DF.groupBy(F.col('Stop Name')).count()\n",
    "lil_arrest_count.printSchema()\n",
    "lil_arrest_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8587f080-b19d-465d-b3df-a9ce6726989e",
   "metadata": {},
   "source": [
    "Testing the haversine function locally and it works so it seems like there's an issue passing the columns? I checked the hw2 submission and it seems fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdcc1c11-aa59-45ca-92af-e8d0480ab715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance = withinMeters(40.799008797000056, -73.95240854099995, 40.816391847000034, -73.89529641399997)\n",
    "distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f6f94-b2ea-4851-be60-f56ced80a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "summons_DF = summons_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"boro\", \"geocoded_column\", \"jurisdiction_code\", \"latitude\", \"law_description\", \"law_section_number\", \"longitude\", \"offense_description\", \"precinct_of_occur\", \"race\", \"sex\", \"summons_category_type\", \"summons_date\", \"summons_key\", \"x_coordinate_cd\", \"y_coordinate_cd\")\\\n",
    "                        .select(\"boro\",\"latitude\", \"longitude\",\"offense_description\",\"summons_category_type\", \"summons_date\", \"summons_key\").filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a060694-f1db-4704-bfde-805fd7ba868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summons_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae800405-d12a-44f9-905a-d76f51f11afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_DF = lil_summons_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"boro\", \"geocoded_column\", \"jurisdiction_code\", \"latitude\", \"law_description\", \"law_section_number\", \"longitude\", \"offense_description\", \"precinct_of_occur\", \"race\", \"sex\", \"summons_category_type\", \"summons_date\", \"summons_key\", \"x_coordinate_cd\", \"y_coordinate_cd\")\\\n",
    "                        .select(\"boro\",\"latitude\", \"longitude\",\"offense_description\",\"summons_category_type\", \"summons_date\", \"summons_key\").filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a8956-0f2d-4efb-83e1-84bffbf4e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_DF.show(truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0228a7ec-fe46-4383-9538-f23d7c0f386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_summons_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88618f9-d013-4891-98af-213e75611fc0",
   "metadata": {},
   "source": [
    "Figuring out the map plotting for the subway locations in geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5fbdb-1761-4529-bbf2-98354af248bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1798ac54-d193-40f2-9334-c5bb144ceba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF = subway_DF.withColumn(\"arrests\", ((F.rand() * 80000) + 20000).cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75713dde-c93a-42ec-b52d-e55d90cb378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97806b08-851f-430f-a4cc-9aeea983673b",
   "metadata": {},
   "source": [
    "Once we have the dataset bounded at <500 rows (a row per station), we should convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f077a2-f482-48a9-9b5f-1e735c51e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF = test_DF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568f676-0bc7-48f8-8ce3-aa3a7f64a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885b2b48-462c-48f4-aa1b-03268860e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF['geometry'] = [Point(xy) for xy in zip(test_DF['GTFS Longitude'],test_DF['GTFS Latitude'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b7715-5fcc-473b-82da-ab08ef4516f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF[\"arrest_pct_total\"] = (test_DF[\"arrests\"] / test_DF[\"arrests\"].sum()) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988d401-9f92-407d-b402-e25b42a81cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF = test_DF.sort_values(by='arrest_pct_total', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff71af3-b18f-47cf-8663-26822ed9fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de97a63-ec6d-4a88-b728-02b952d95ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_DF[\"Borough\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714db4b6-c745-449d-83c6-965a2c74c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_map = gpd.read_file(r'C:\\Users\\Nigel\\Github\\big-data-project\\data\\nynta2020_23a\\nynta2020.shp')\n",
    "nta_map.to_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c5499f-5653-4d5d-b5ae-97bcab192bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geo_DF = gpd.GeoDataFrame(test_DF, crs=4326, geometry = test_DF.geometry)\n",
    "# Just to be extra sure\n",
    "test_geo_DF.to_crs(4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31b9d3-f60c-4955-b181-83fbf67c6fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,7))\n",
    "nta_map.boundary.plot(ax=ax, edgecolor='k');\n",
    "test_geo_DF[test_geo_DF.Borough == 'M']\\\n",
    "            .head(10)\\\n",
    "            .plot(column='Stop Name', ax=ax, legend=True, marker='.',\\\n",
    "            markersize=test_geo_DF.arrest_pct_total.astype('float') * 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53337ace-4461-4201-8c0a-bceb340306b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_test_geo_DF = test_geo_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9e961-519b-4ccc-a941-a4ebe3a7953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_demos = pandas.read_excel('https://www1.nyc.gov/assets/planning/download/office/planning-level/nyc-population/acs/demo_2019_acs5yr_nta.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b55b7f3-97c1-4830-b15f-394b634fb968",
   "metadata": {},
   "outputs": [],
   "source": [
    "nta_df = nta_map.merge(nta_demos, how='left', left_on='NTA2020', right_on='GeoID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9775b8-3803-4cd8-a0d8-69ba24daac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_test_geo_DF.to_file(\"test_geo.json\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee5a74d-f9d0-48e4-b04f-aacb05205450",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(center=(40,-100),zoom=4)\n",
    "m.add_gdf(nta_df, layer_name='2020 NTA Demographic Information', info_mode='on_click')\n",
    "m.add_point_layer(filename=r'C:\\Users\\Nigel\\Github\\big-data-project\\test_geo.json', popup=['Stop Name', 'arrests', 'arrest_pct_total'], layer_name=\"Stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563e154-e29c-4711-8bf1-9bc7407765c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd11008-250b-43d3-a12a-16f9f3a19590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
