{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip\n",
    "# !pip install tqdm\n",
    "# !pip install dask\n",
    "# !pip install apache-sedona\n",
    "# !pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask\n",
    "#!conda install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ce98286-fb6b-4af9-994f-689a6f13d838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sedona.register import SedonaRegistrator  \n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from shapely.geometry import Point\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from haversine import haversine, Unit\n",
    "import pyspark.sql.types as types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import pyspark\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0745851c-98cb-468f-8422-527aa52bcb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://workHorse:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[7]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11d65e14790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    "    .setMaster(\"local[7]\")\\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .set(\"spark.eventLog.dir\", \"./logs\")\\\n",
    "    .set(\"spark.eventLog.gcMetrics.youngGenerationGarbageCollectors\", \"true\")\\\n",
    "    .set(\"spark.executor.heartbeatInterval\",\"3600s\")\\\n",
    "    .set(\"spark.network.timeout\",\"3601s\")\n",
    "    # .set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\\\n",
    "    # .set(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName)\\\n",
    "    # .set('spark.jars.packages',\n",
    "           # 'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n",
    "           # 'org.datasyslab:geotools-wrapper:1.4.0-28.2')\n",
    "\n",
    "'''\n",
    "sparkSession = SparkSession. \\\n",
    "    builder. \\\n",
    "    appName('appName'). \\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName). \\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName). \\\n",
    "    config('spark.jars.packages',\n",
    "           'org.apache.sedona:sedona-spark-shaded-3.0_2.12:1.4.0,'\n",
    "           'org.datasyslab:geotools-wrapper:1.4.0-28.2'). \\\n",
    "    getOrCreate()\n",
    "'''\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel('ERROR')\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b68d9-800f-400e-ae44-196e39a7680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SedonaRegistrator.registerAll(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=10', 'filename': 'lil_arrest.json' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=15000000', 'filename': 'summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/i55r-43gk.json?$limit=15000000', 'filename': 'turnstile.json' }\n",
    "#subway_loc = { 'url': 'https://data.ny.gov/resource/i9wp-a4ja.json?$limit=15000000', 'filename': 'subway.json' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=15000000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "         if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "subway.csv: 65.5kB [00:00, 577kB/s] \n",
      "subway.csv: 65.5kB [00:00, 560kB/s]"
     ]
    }
   ],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes\n",
    "arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)\n",
    "complaint_rdd = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True)\n",
    "summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)\n",
    "turnstile_rdd = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True)\n",
    "subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2166298f-6ea3-401b-bcbd-603cdd65b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_rdd = spark.read.json(os.path.join(data_dir, subway_loc['filename']), multiLine=True)\n",
    "subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a412d-5a3b-45cd-a0f5-fa61a9c6c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d12d731d-0181-4fb7-a41b-6ee0121047bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55b29ff0-4812-4fd5-9e3d-267f51352243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subway_DF = subway_rdd.toDF(\"ada\", \"ada_notes\", \"corner\", \"division\", \"east_west_street\", \"entrance_georeference\", \"entrance_latitude\", \"entrance_location\", \"entrance_longitude\", \"entrance_type\", \"entry\", \"exit_only\", \"free_crossover\", \"line\", \"north_south_street\", \"route1\", \"route10\", \"route11\", \"route2\", \"route3\", \"route4\", \"route5\", \"route6\", \"route7\", \"route8\", \"route9\", \"staff_hours\", \"staffing\", \"station_georeference\", \"station_latitude\", \"station_location\", \"station_longitude\", \"station_name\", \"vending\")\n",
    "subway_DF = subway_rdd.toDF(\"Station ID\", \"Complex ID\", \"GTFS Stop ID\", \"Division\", \"Line\", \"Stop Name\", \"Borough\", \"Daytime Routes\", \"Structure\", \"GTFS Latitude\", \"GTFS Longitude\", \"North Direction Label\", \"South Direction Label\", \"ADA\", \"ADA Direction Notes\", \"ADA NB\", \"ADA SB\", \"Capital Outage NB\", \"Capital Outage SB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f05737b4-f6bf-4d07-9077-a25f5c08ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+--------+-------+--------------------+-------+--------------+---------+-------------+--------------+---------------------+---------------------+---+-------------------+------+------+-----------------+-----------------+\n",
      "|Station ID|Complex ID|GTFS Stop ID|Division|   Line|           Stop Name|Borough|Daytime Routes|Structure|GTFS Latitude|GTFS Longitude|North Direction Label|South Direction Label|ADA|ADA Direction Notes|ADA NB|ADA SB|Capital Outage NB|Capital Outage SB|\n",
      "+----------+----------+------------+--------+-------+--------------------+-------+--------------+---------+-------------+--------------+---------------------+---------------------+---+-------------------+------+------+-----------------+-----------------+\n",
      "|         1|         1|         R01|     BMT|Astoria|Astoria-Ditmars Blvd|      Q|           N W| Elevated|    40.775036|    -73.912034|                 null|            Manhattan|  0|               null|  null|  null|             null|             null|\n",
      "|         2|         2|         R03|     BMT|Astoria|        Astoria Blvd|      Q|           N W| Elevated|    40.770258|    -73.917843|         Ditmars Blvd|            Manhattan|  1|               null|  null|  null|             null|             null|\n",
      "+----------+----------+------------+--------+-------+--------------------+-------+--------------+---------+-------------+--------------+---------------------+---------------------+---+-------------------+------+------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_DF.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b340e07-b3eb-495f-99e5-6872631915fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Station ID',\n",
       " 'Complex ID',\n",
       " 'GTFS Stop ID',\n",
       " 'Division',\n",
       " 'Line',\n",
       " 'Stop Name',\n",
       " 'Borough',\n",
       " 'Daytime Routes',\n",
       " 'Structure',\n",
       " 'GTFS Latitude',\n",
       " 'GTFS Longitude',\n",
       " 'North Direction Label',\n",
       " 'South Direction Label',\n",
       " 'ADA',\n",
       " 'ADA Direction Notes',\n",
       " 'ADA NB',\n",
       " 'ADA SB',\n",
       " 'Capital Outage NB',\n",
       " 'Capital Outage SB']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e1fe1fa-8c11-4996-b464-d3187618b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_DF = subway_DF.select(\"station_name\",\"station_latitude\", \"station_longitude\", \"station_location\", \"station_georeference\")\n",
    "subway_DF = subway_DF.select('Station ID', 'Complex ID', 'GTFS Stop ID', 'Stop Name', 'Borough', 'GTFS Latitude','GTFS Longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "43b63eb5-eb6f-48d3-a646-1c50e101f9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------------+--------------------+-------+-------------+--------------+\n",
      "|Station ID|Complex ID|GTFS Stop ID|           Stop Name|Borough|GTFS Latitude|GTFS Longitude|\n",
      "+----------+----------+------------+--------------------+-------+-------------+--------------+\n",
      "|         1|         1|         R01|Astoria-Ditmars Blvd|      Q|    40.775036|    -73.912034|\n",
      "|         2|         2|         R03|        Astoria Blvd|      Q|    40.770258|    -73.917843|\n",
      "|         3|         3|         R04|               30 Av|      Q|    40.766779|    -73.921479|\n",
      "|         4|         4|         R05|            Broadway|      Q|     40.76182|    -73.925508|\n",
      "|         5|         5|         R06|               36 Av|      Q|    40.756804|    -73.929575|\n",
      "|         6|         6|         R08|   39 Av-Dutch Kills|      Q|    40.752882|    -73.932755|\n",
      "|         7|       613|         R11|  Lexington Av/59 St|      M|     40.76266|    -73.967258|\n",
      "|         8|         8|         R13|          5 Av/59 St|      M|    40.764811|    -73.973347|\n",
      "|         9|         9|         R14|          57 St-7 Av|      M|    40.764664|    -73.980658|\n",
      "|        10|        10|         R15|               49 St|      M|    40.759901|    -73.984139|\n",
      "|        11|       611|         R16|      Times Sq-42 St|      M|    40.754672|    -73.986754|\n",
      "|        12|       607|         R17|     34 St-Herald Sq|      M|    40.749567|     -73.98795|\n",
      "|        13|        13|         R18|               28 St|      M|    40.745494|    -73.988691|\n",
      "|        14|        14|         R19|               23 St|      M|    40.741303|    -73.989344|\n",
      "|        15|       602|         R20|      14 St-Union Sq|      M|    40.735736|    -73.990568|\n",
      "|        16|        16|         R21|            8 St-NYU|      M|    40.730328|    -73.992629|\n",
      "|        17|        17|         R22|           Prince St|      M|    40.724329|    -73.997702|\n",
      "|        18|       623|         R23|            Canal St|      M|    40.719527|    -74.001775|\n",
      "|        19|       623|         Q01|            Canal St|      M|    40.718383|     -74.00046|\n",
      "|        20|        20|         R24|           City Hall|      M|    40.713282|    -74.006978|\n",
      "+----------+----------+------------+--------------------+-------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5997b50-94f7-4db6-8bdc-85898cd68bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station ID: integer (nullable = true)\n",
      " |-- Complex ID: integer (nullable = true)\n",
      " |-- GTFS Stop ID: string (nullable = true)\n",
      " |-- Stop Name: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- GTFS Latitude: double (nullable = true)\n",
      " |-- GTFS Longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subway_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ac8544-d683-4905-8e60-c3e733e64052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18cd171-fa22-47ab-b0bc-d18c3d3a9ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.dropDuplicates(subset=[\"Stop Name\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b94f75-2417-4ee8-8686-6e79905352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSIDER DROPPING SUBWAY STOPS WITH DUPLICATE NAMES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62709978-b51f-4f1f-beb7-28f0b1e3346e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subway_DF = subway_DF.withColumn(\"GeoPoint\", F.struct(\"GTFS Latitude\", \"GTFS Longitude\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "775bd688-9fb4-405d-858b-faec2a39910a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def withinMeters(slat, slong, dlat, dlong):\n",
    "    srs = (slat, slong)\n",
    "    dst = (dlat, dlong)\n",
    "    print(type(srs[0]),type(srs[1]))\n",
    "    print(srs[0],srs[1])\n",
    "    print(type(dst[0]),type(dst[1]))\n",
    "    print(dst[0],dst[1])\n",
    "    distance = float(haversine(srs, dst,unit=Unit.METERS))\n",
    "    print(distance)\n",
    "    return bool(distance < 402)\n",
    "    \n",
    "withinMetersUdf = F.udf(withinMeters, types.BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f96a13a-bfd8-4c86-b554-6ab4c91bfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_DF = arrest_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a392c299-4d3b-4a73-b575-8c0a52537550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrest_DF = arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", \"lon_lat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedde2e-acaf-46b0-b264-8ff9738b8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_DF = arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d554d88-706f-434e-8d77-fe974b8beb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce390a10-6d13-4b06-8468-f6cc0a14de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF = subway_DF.join(arrest_DF, withinMetersUdf('GTFS Latitude', 'GTFS Longitude', 'latitude', 'longitude'), 'cross')\\\n",
    "    .drop(F.col('latitude'))\\\n",
    "    .drop(F.col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26f802-ec34-4ae7-9c41-f34c08667daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f23ade-7cc1-47ce-a089-90cf13626213",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF = subway_DF.na.drop(subset=[\"Stop Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f7b8c-773f-485b-9f82-dac20090e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_count = subway_DF.groupBy(F.col('Stop Name')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec9116-15e2-45b1-b860-9c20460ff632",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_count = arrest_count.dropDuplicates(subset=[\"Stop Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577651bf-7134-4bdc-9d78-2eeccc680e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_count.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c0c78-12dc-422b-a189-76b579506748",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrest_count.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5713f17-cf92-4d6e-817c-4430525dccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF = subway_DF.na.drop(subset=[\"GTFS Latitude\"])\n",
    "subway_DF = subway_DF.na.drop(subset=[\"GTFS Longitude\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da085d-bdad-4761-b34e-6a3f6f9e80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "209d1f21-ef7c-42e4-86fc-5b80659d33f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF = lil_arrest_rdd.toDF(\":@computed_region_92fq_4b7q\", \":@computed_region_efsh_h5xi\", \":@computed_region_f5dn_yrer\", \":@computed_region_sbqj_enih\", \":@computed_region_yeji_bk3q\", \"age_group\", \"arrest_boro\", \"arrest_date\", \"arrest_key\", \"arrest_precinct\", \"jurisdiction_code\", \"ky_cd\", \"latitude\", \"law_cat_cd\", \"law_code\", \"lon_lat\", \"longitude\", \"ofns_desc\", \"pd_cd\", \"pd_desc\", \"perp_race\", \"perp_sex\", \"x_coord_cd\", \"y_coord_cd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "35367b0b-8769-4476-a461-0dcafedcbf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF = lil_arrest_DF.select(\"arrest_boro\",\"arrest_date\", \"arrest_key\", \"latitude\", \"longitude\", ).filter(F.col('latitude').isNotNull() & F.col('longitude').isNotNull()).withColumn(\"longitude\", F.col(\"longitude\").cast(\"double\")).withColumn(\"latitude\", F.col(\"latitude\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72d5ae1e-11b0-4ebc-99f1-7bc311c83cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+------------------+------------------+\n",
      "|arrest_boro|         arrest_date|arrest_key|          latitude|         longitude|\n",
      "+-----------+--------------------+----------+------------------+------------------+\n",
      "|          M|2021-11-22T00:00:...| 236791704|40.799008797000056|-73.95240854099995|\n",
      "|          B|2021-12-04T00:00:...| 237354740|40.816391847000034|-73.89529641399997|\n",
      "|          Q|2021-11-09T00:00:...| 236081433| 40.67970040800003|-73.77604736799998|\n",
      "|          M|2019-01-26T00:00:...| 192799737|40.800694331000045|-73.94110928599997|\n",
      "|          M|2019-02-06T00:00:...| 193260691| 40.75783900300007|-73.99121211099998|\n",
      "|          Q|2021-12-03T00:00:...| 237291769| 40.77205649600006|-73.87622400099998|\n",
      "|          B|2021-11-10T00:00:...| 236106641|40.804012949000025|-73.87833183299993|\n",
      "|          Q|2021-12-28T00:00:...| 238383628| 40.69166001700007|-73.77919852099996|\n",
      "|          K|2016-01-06T00:00:...| 149117452|40.648650085000035|-73.95033556299995|\n",
      "+-----------+--------------------+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lil_arrest_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a411b304-f06c-40d7-a43e-7e4b67bd183b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrest_boro: string (nullable = true)\n",
      " |-- arrest_date: string (nullable = true)\n",
      " |-- arrest_key: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lil_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "50cf9c26-bc9c-4319-8405-de6180cb5444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+------------------+------------------+\n",
      "|arrest_boro|         arrest_date|arrest_key|          latitude|         longitude|\n",
      "+-----------+--------------------+----------+------------------+------------------+\n",
      "|          M|2021-11-22T00:00:...| 236791704|40.799008797000056|-73.95240854099995|\n",
      "|          B|2021-12-04T00:00:...| 237354740|40.816391847000034|-73.89529641399997|\n",
      "|          Q|2021-11-09T00:00:...| 236081433| 40.67970040800003|-73.77604736799998|\n",
      "|          M|2019-01-26T00:00:...| 192799737|40.800694331000045|-73.94110928599997|\n",
      "|          M|2019-02-06T00:00:...| 193260691| 40.75783900300007|-73.99121211099998|\n",
      "|          Q|2021-12-03T00:00:...| 237291769| 40.77205649600006|-73.87622400099998|\n",
      "|          B|2021-11-10T00:00:...| 236106641|40.804012949000025|-73.87833183299993|\n",
      "|          Q|2021-12-28T00:00:...| 238383628| 40.69166001700007|-73.77919852099996|\n",
      "|          K|2016-01-06T00:00:...| 149117452|40.648650085000035|-73.95033556299995|\n",
      "+-----------+--------------------+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lil_arrest_DF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a52d7306-8b04-4bb4-8c4b-4810efb10063",
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_DF = subway_DF.join(lil_arrest_DF, withinMetersUdf(F.col('GTFS Latitude'), F.col('GTFS Longitude'), F.col('latitude'), F.col('longitude')), 'cross')\\\n",
    "    .drop(F.col('latitude'))\\\n",
    "    .drop(F.col('longitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3427eeaa-d965-4265-81d1-b27ea3542217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Stop Name: string (nullable = true)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o333.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 37) (workHorse executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[72], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m lil_arrest_count \u001b[38;5;241m=\u001b[39m subway_DF\u001b[38;5;241m.\u001b[39mgroupBy(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStop Name\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mcount()\n\u001b[0;32m      3\u001b[0m lil_arrest_count\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mlil_arrest_count\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\sql\\dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    896\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o333.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 37) (workHorse executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:192)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:166)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:82)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:853)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:853)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/sun.nio.ch.NioSocketImpl.timedAccept(NioSocketImpl.java:694)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.accept(NioSocketImpl.java:738)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:690)\r\n\tat java.base/java.net.ServerSocket.platformImplAccept(ServerSocket.java:655)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:631)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:588)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:546)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:179)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "subway_DF = subway_DF.na.drop(subset=[\"Stop Name\"])\n",
    "lil_arrest_count = subway_DF.groupBy(F.col('Stop Name')).count()\n",
    "lil_arrest_count.printSchema()\n",
    "lil_arrest_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c8c66-f547-44ec-b74a-2a1eb46bb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "lil_arrest_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "404c6dc6-5774-4919-91d8-62cd7e6c010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arrest_boro: string (nullable = true)\n",
      " |-- arrest_date: string (nullable = true)\n",
      " |-- arrest_key: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lil_arrest_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cdcc1c11-aa59-45ca-92af-e8d0480ab715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'> <class 'float'>\n",
      "40.799008797000056 -73.95240854099995\n",
      "<class 'float'> <class 'float'>\n",
      "40.816391847000034 -73.89529641399997\n",
      "5180.8801108369735\n"
     ]
    }
   ],
   "source": [
    "distance = withinMeters(40.799008797000056, -73.95240854099995, 40.816391847000034, -73.89529641399997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fb74545d-f7e2-44e9-9a62-0dd54ceb59c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subway_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f6f94-b2ea-4851-be60-f56ced80a652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
