{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be5532d-f960-4781-9bcd-46e9fc649520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip\n",
    "# !pip install tqdm\n",
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1161ce3c-f44d-4315-a465-e24bb275488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda\n",
    "# !conda install tqdm\n",
    "# !conda install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c7e655-046c-4da4-a507-d6c82a395dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration, worked on using python@3.10.9 \n",
    "import os\n",
    "import urllib\n",
    "import json\n",
    "from threading import Thread, Lock\n",
    "from tqdm import tqdm\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import pyspark\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0745851c-98cb-468f-8422-527aa52bcb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/26 19:00:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'setLogLevel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m sc \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39mSparkContext(conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m pyspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mSparkSession(sc)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetLogLevel\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m spark\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'setLogLevel'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 19:00:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "23/04/26 19:00:11 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC, G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "conf = pyspark.SparkConf()\\\n",
    "    .setMaster(\"local[7]\")\\\n",
    "    .set(\"spark.eventLog.enabled\", \"true\")\\\n",
    "    .set(\"spark.eventLog.dir\", \"./logs\")\\\n",
    "    .set(\"spark.eventLog.gcMetrics.youngGenerationGarbageCollectors\", \"true\").setLogLevel('ERROR')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sc.setLogLevel\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78fd043a-320a-4e86-86a8-df4b0b177f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "data_dir = 'data'\n",
    "\n",
    "# data urls\n",
    "historic_arrest_loc = { 'url': 'https://data.cityofnewyork.us/resource/8h9b-rp9u.json?$limit=15000000', 'filename': 'arrest.json' }\n",
    "historic_complaint_loc = { 'url': 'https://data.cityofnewyork.us/resource/qgea-i56i.json?$limit=15000000', 'filename': 'complaint.json' }\n",
    "historic_court_summons_loc = { 'url': 'https://data.cityofnewyork.us/resource/sv2w-rv3k.json?$limit=15000000', 'filename': 'summons.json' }\n",
    "traffic_speed_loc = { 'url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=15000000', 'filename': 'speed.json' }\n",
    "turnstile_loc = { 'url': 'https://data.ny.gov/resource/i55r-43gk.json?$limit=15000000', 'filename': 'turnstile.json' }\n",
    "subway_loc = { 'url': 'http://web.mta.info/developers/data/nyct/subway/Stations.csv?$limit=10000', 'filename': 'subway.csv' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296b368d-abad-457e-9273-a4d58bb0164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download flags\n",
    "downloadflag = True\n",
    "redownload = False\n",
    "\n",
    "thread_lock = Lock()\n",
    "\n",
    "# download utils\n",
    "def download_dataset_thread(loc, folder):\n",
    "    with thread_lock:\n",
    "         if ((not os.path.exists(os.path.join(folder, loc['filename']))) or redownload) and downloadflag:\n",
    "            if os.path.isfile(os.path.join(folder, loc['filename'])):\n",
    "                os.remove(os.path.join(folder, loc['filename']))\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder) \n",
    "            with tqdm(unit=\"B\", unit_scale=True, desc=loc['filename'], miniters=1) as progress_bar:\n",
    "                urllib.request.urlretrieve(loc['url'], os.path.join(folder, loc['filename']), lambda block_num, block_size, total_size: progress_bar.update(block_size))\n",
    "            progress_bar.display()\n",
    "        \n",
    "def download_dataset(loc, folder):\n",
    "    thread = Thread(target=download_dataset_thread, args=(loc, folder))\n",
    "    thread.start()\n",
    "    thread.join()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bf5d47c-e667-4356-a238-d10729cb744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download datasets\n",
    "for dataset in [historic_arrest_loc,\n",
    "                historic_complaint_loc,\n",
    "                historic_court_summons_loc,\n",
    "                turnstile_loc,\n",
    "                subway_loc]:\n",
    "    download_dataset(dataset, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a03187c4-9ff0-4728-8926-3bcdda56315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 18:42:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "23/04/26 18:42:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC, G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "23/04/26 18:42:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Old Generation, G1 Concurrent GC, G1 Young Generation), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dataframes\n",
    "arrest_rdd = spark.read.json(os.path.join(data_dir, historic_arrest_loc['filename']), multiLine=True)\n",
    "complaint_rdd = spark.read.json(os.path.join(data_dir, historic_complaint_loc['filename']), multiLine=True)\n",
    "summons_rdd = spark.read.json(os.path.join(data_dir, historic_court_summons_loc['filename']), multiLine=True)\n",
    "turnstile_rdd = spark.read.json(os.path.join(data_dir, turnstile_loc['filename']), multiLine=True)\n",
    "subway_rdd = spark.read.csv(os.path.join(data_dir, subway_loc['filename']), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b29ff0-4812-4fd5-9e3d-267f51352243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
